[{"categories":["Tech"],"date":"2024-05-30T00:00:00Z","description":"2024-05-30","keywords":null,"link":"/blog/2024-05-30-memx-memory-usage-linux/","tags":["process","memory","usage","linux","golang"],"text":"I shared a simple piece of code for getting a process‚Äô memory usage in Linux. It‚Äôs called memx. It‚Äôs Linux-specific only as it reads the proportional set size (PSS) data from either /proc/{pid}/smaps_rollup (if present) or /proc/{pid}/smaps file. I‚Äôve used this piece of code many times at work. We use memory-mapped files extensively in some of our services and this is how we get more accurate results. Very useful in debugging OOMKilled events in k8s.","title":"memx - Get process‚Äô memory usage (Linux)"},{"categories":["Tech"],"date":"2024-05-13T00:00:00Z","description":"2024-05-13","keywords":null,"link":"/blog/2024-05-13-the-divs-model/","tags":["process","startup","agile","development"],"text":"I posted a blog introducing the DIVS model, the process we use at Alphaus, the startup I work for. Check it out here.","title":"The DIVS model"},{"categories":["Tech"],"date":"2024-05-03T00:00:00Z","description":"2024-05-03","keywords":null,"link":"/blog/2024-05-03-oomkill-watch/","tags":["k8s","oomkilled","events","watch","kubectl","golang"],"text":"I recently uploaded a tool to GitHub that wraps the kubectl get events -w command for watching OOMKilled events in Kubernetes. It‚Äôs called oomkill-watch. You can check out the code here. You might find this useful.","title":"oomkill-watch - A tool to watch OOMKilled events in k8s"},{"categories":["content","paige"],"date":"2024-01-18T21:32:52-07:00","description":"2024-01-18: The ultimate guide to markdown syntax in paige.","keywords":null,"link":"/tmp/markdown-syntax/","tags":["markdown","css","html"],"text":" This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme. Headings The following HTML \u003ch2\u003e‚Äî\u003ch6\u003e elements represent five levels of section headings. \u003ch2\u003e is the highest section level while \u003ch6\u003e is the lowest. H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat. Itatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat. Blockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations. Blockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote. Blockquote with attribution Don‚Äôt communicate by sharing memory, share memory by communicating. ‚Äî Rob Pike1 Tables Tables aren‚Äôt part of the core Markdown spec, but Hugo supports supports them out-of-the-box. Name Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code Code Blocks Code block with backticks \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Code block indented with four spaces \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Code block with Hugo‚Äôs internal highlight shortcode \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements ‚Äî abbr, sub, sup, kbd, mark GIF is a bitmap image format. H2O Xn + Yn = Zn Press CTRL+ALT+Delete to end the session. Most salamanders are nocturnal, and hunt for insects, worms, and other small creatures. The above quote is excerpted from Rob Pike‚Äôs talk during Gopherfest, November 18, 2015.¬†‚Ü©Ô∏é","title":"Markdown Syntax Guide"},{"categories":["content","paige"],"date":"2023-09-24T21:29:31-07:00","description":"Only a video.","keywords":null,"link":"/tmp/video/","tags":["video"],"text":"","title":"Video"},{"categories":["content","paige"],"date":"2023-09-24T21:29:30-07:00","description":"A front matter link.","keywords":null,"link":"/tmp/link/","tags":["link"],"text":"It takes you to the home page.","title":"Link"},{"categories":["content","paige"],"date":"2023-09-18T21:33:24-07:00","description":"A brief description of Hugo Shortcodes.","keywords":null,"link":"/tmp/rich-content/","tags":["shortcodes","privacy"],"text":" Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds. YouTube Privacy Enhanced Shortcode Twitter Simple Shortcode ‚ÄúIn addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.‚Äù ‚Äî Jan Tschichold pic.twitter.com/gcv7SrhvJb ‚Äî Design Reviewed | Graphic Design History (@DesignReviewed) January 17, 2019 Vimeo Simple Shortcode","title":"Rich Content"},{"categories":["content","paige"],"date":"2023-09-18T21:33:16-07:00","description":"Lorem Ipsum Dolor Si Amet.","keywords":null,"link":"/tmp/placeholder-text/","tags":["markdown","text"],"text":" Lorem est tota propiore conpellat pectoribus de pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum. Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis. Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon Mane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis. Iubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et. Eurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.","title":"Placeholder Text"},{"categories":["content","paige"],"date":"2023-09-18T21:33:03-07:00","description":"A brief guide to setup KaTeX.","keywords":null,"link":"/tmp/math-typesetting/","tags":["katex","math","typesetting"],"text":" Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries. In this example we will be using KaTeX Create a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: {{ if .Params.paige.math }} {{ partial \"math.html\" . }} {{ end }} To enable KaTex on a per page basis include the parameter paige.math: true in content files Note: Use the online reference of Supported TeX Functions Examples Inline math: \\(\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887‚Ä¶\\) Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","title":"Math Typesetting"},{"categories":["content","paige"],"date":"2023-09-18T21:32:40-07:00","description":"Guide to emoji usage in Hugo.","keywords":null,"link":"/tmp/emoji-support/","tags":["emoji"],"text":" Emoji can be enabled in a Hugo project in a number of ways. The emojify function can be called directly in templates or Inline Shortcodes. To enable emoji globally, set enableEmoji to true in your site‚Äôs configuration and then you can type emoji shorthand codes directly in content files; e.g. üôà :see_no_evil: üôâ :hear_no_evil: üôä :speak_no_evil: The Emoji cheat sheet is a useful reference for emoji shorthand codes. N.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g. .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; }","title":"Emoji Support"},{"categories":["content","paige"],"date":"2023-09-18T21:21:05-07:00","description":"An alert.","keywords":null,"link":"/tmp/alert/","tags":["alerts"],"text":"This page has the following parameters: paige: alert: message: \"Get more information \u003ca href=\\\"#\\\" class=\\\"alert-link\\\"\u003ehere\u003c/a\u003e.\" type: \"primary\"","title":"Alert"},{"categories":["Tech"],"date":"2023-08-09T00:00:00Z","description":"2023-08-09","keywords":null,"link":"/blog/2023-08-09-cto-diaries-on-not-shipping-your-org-chart/","tags":["cto-diaries","cto","diaries","conways-law"],"text":"You‚Äôve probably heard of the warning ‚ÄúDon‚Äôt ship the org chart‚Äù common among product development circles. I always thought of this as synonymous to Conway‚Äôs Law which states that organizations design systems that mirror their own communication structure. In my experience, I‚Äôve come to believe that this is true. Whether you like it or not, it is an eventuality. At some point I thought that to be an effective solutions architect, you have to be an ‚Äúorg architect‚Äù. Or in order to achieve a certain system architecture, you‚Äôre better off rearranging your org structure than influence cross-functional architects or senior engineering leadership to agree with you. It may work for a time, especially during the start, but eventually, as engineers come and go, the org-mirrored design will out. But I think there‚Äôs more nuance to this than what‚Äôs obvious. There are several types of org structures but I will cover only what‚Äôs relevant to my situation which is startups. This might be anecdotal but between peers, the most common types fall under these two broad classifications: less boundaries or bigger teams, and smaller, often siloed teams. Teams with less boundaries tend to move slower because more people need to communicate for alignment but will produce a more coherent product. Smaller teams on the other hand, move faster but their outputs are more difficult to integrate together, resulting in an inconsistent product experience. Personally, I don‚Äôt really have a strong opinion on which is better. I think there are phases in a startup where one works better than the other. But this requires you to understand what you want to build in the first place and then build your org chart around that. There‚Äôs also the case of building your org structure based on product lines if you have multiple products as opposed to functional teams covering multiple products. Product-based org structures might promote autonomy and quicker response to industry changes but could easily lead to system duplication (wasted resources). Functional structure however, encourages specialization and focus but inhibits multi-boundary communications. You may argue that, ‚ÄúDoes it really matter? Customers don‚Äôt really care about how your org is structured as long as your product is usable and has great UX.‚Äù I would actually agree that this is a good baseline for structuring your org. You will have several vertical teams per product but an overlapping horizontal team that ensures UX coherence, usability, and branding. I think this is what the Mirroring Hypothesis study (confirms Conway‚Äôs Law) refers to as partial mirroring; in which technological knowledge are invested, shared, and acquired beyond operational boundaries. ‚ÄúAPI-first‚Äù companies come to mind as in partial mirroring, building contractual relationships (APIs) that support technical interdependency across boundaries seems to work. And more often than not, your org structure will undergo changes multiple times during your product‚Äôs lifetime, which will result into subsequent changes to your system‚Äôs design that will then mirror the new org, and so on and so forth, so you might as well embrace this dynamism instead of fighting against it. Finally, the study also highlights collaboration patterns in the open source sphere that do not support the mirroring hypothesis (and thus, Conway‚Äôs Law). My experience with OSS collaboration is mostly outside work so I‚Äôm interested to know if there is a case for adopting OSS-style structure within the organization. I think that‚Äôs a good topic for another day. To conclude, instead of going with the warning ‚Äúdon‚Äôt ship the org chart‚Äù, since you will ship your org chart anyway, be aware of it, understand it, and make sure it works on your favor.","title":"CTO Diaries #4: On not shipping your org chart"},{"categories":["Tech"],"date":"2023-05-29T00:00:00Z","description":"2023-05-29","keywords":null,"link":"/blog/2023-05-29-announcing-octo/","tags":["product","release","alphaus","octo"],"text":"We just recently announced the public beta of our new product, OCTO. If you‚Äôre interested, you can join our waiting list at https://lp.alphaus.cloud/waitlist.","title":"Announcing our new product, OCTO"},{"categories":["Tech"],"date":"2023-05-11T00:00:00Z","description":"2023-05-11","keywords":null,"link":"/blog/2023-05-11-retries-backoff/","tags":["retry","retries","backoff","distributed-systems"],"text":"In a distributed system, where multiple processes communicate with each other over a network, failures are inevitable. Network partitions, hardware failures, and software bugs can all cause a request to fail. Retries with backoff are a critical technique to help mitigate these failures. Retries refer to the act of retrying a failed request. When a request fails, the client can retry the request, hoping that it will succeed the next time around. However, simply retrying the request immediately after a failure can be problematic. If the failure was caused by a temporary network issue, for example, retrying immediately will likely result in another failure. This is where backoff comes in. Backoff refers to the practice of waiting a certain amount of time before retrying a failed request. The idea is to wait long enough for any temporary issues to resolve themselves before retrying. The amount of time to wait is typically increased with each retry, hence the term ‚Äúbackoff.‚Äù The idea is that if the request fails multiple times, the client will eventually back off enough to give the system a chance to recover. There are several benefits to using retries with backoff in a distributed system. First, it can help reduce the impact of temporary failures. By waiting before retrying a request, the client can avoid bombarding the target with requests, which can exacerbate the problem. Second, it can help improve overall system availability. By retrying failed requests, the client can work around transient issues that might otherwise cause the entire system to fail. There are several strategies for implementing retries with backoff. One common approach is exponential backoff, where the client waits an increasing amount of time between each retry. Another approach is jittered backoff, where the client adds a random amount of time to the wait period to avoid the so-called ‚Äúthundering herd‚Äù problem, where multiple clients all retry at the same time. Example 1: import ( ... backoffv4 \"github.com/cenkalti/backoff/v4\" ) func main() { var n int operation := func() error { n++ log.Printf(\"n=%v\\n\", n) if n \u003e= 10 { return nil } return fmt.Errorf(\"backoff\") } err := backoffv4.Retry(operation, backoffv4.NewExponentialBackOff()) if err != nil { log.Println(\"final backoff failed\") } } Example 2 (my preference): import ( ... gaxv2 \"github.com/googleapis/gax-go/v2\" ) func main() { bo := gaxv2.Backoff{ Initial: time.Second, Max: time.Minute, } var n int operation := func() error { n++ log.Printf(\"cnt=%v\\n\", n) if n \u003e= 10 { return nil } return fmt.Errorf(\"backoff\") } for { err := operation() if err != nil { time.Sleep(bo.Pause()) continue } break } } In conclusion, retries with backoff are an important technique for improving the robustness and availability of distributed systems. By waiting before retrying failed requests, the client can help reduce the impact of temporary failures and improve overall system availability. There are several strategies for implementing retries with backoff, and choosing the right approach will depend on the specific requirements of the system. Additional reading: https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/","title":"Retries with backoff in distributed systems"},{"categories":["Tech"],"date":"2023-05-03T00:00:00Z","description":"2023-05-03","keywords":null,"link":"/blog/2023-05-03-alphaus-blue-api/","tags":["alphaus","blog","engineering","technical"],"text":"Hey there, I just posted a blog about gRPC here. If gRPC and grpc-gateway is right up your alley, you might find it interesting.","title":"Alphaus Blue API"},{"categories":["Tech"],"date":"2023-04-28T00:00:00Z","description":"2023-04-28","keywords":null,"link":"/blog/2023-04-28-hedge-memberlist/","tags":["memberlist","hedge","distributed-computing","k8s","kubernetes"],"text":"I recently came across the hashicorp/memberlist library while browsing GitHub and I thought it would be a good replacement for hedge‚Äôs internal member tracking logic. It seems to be widely used (thus more battle-tested) as well. I was quite excited as I always thought that hedge‚Äôs equivalent logic is too barebones and untested outside of our use cases. It works just fine for its current intended purpose but I‚Äôve been hesitating to build on top of it until I can really say that it‚Äôs stable enough. With memberlist, it might just be what I needed. After about a month of testing, I think it didn‚Äôt really turn out quite well in the end. It is stable enough for deployments that are not spike-y in terms of workloads (frequent scaling up/down). Or if I set min = max in the HorizontalPodAutoscaler. In these cases, memberlist can consistently track members just fine. What‚Äôs better is that it works even in multiple deployments in the same namespace which I thought was brilliant. For example, if I have a deployment app1 set to 10 pods in the default namespace using memberlist‚Äôs default port numbers, and then I deploy another set, say, app2, within the same namespace using the same ports, app1‚Äôs memberlist can track its 10 member pods just fine while app2‚Äôs memberlist is also separated. But when applied to my use case, which has a minimum pod of 2 and a max of 150, with frequent scale up/down frequency depending on load, it can‚Äôt seem to keep up. The potential for Byzantine faults is just too high: i.e. in a 50-pod scale, memberlist can end up having 2 groups of m-pods and n-pods where m+n=50. Very rarely, it can even go up to 3 groups. I am a little frustrated. I really wanted it to work; I even attempted to update memberlist to incorporate hedge‚Äôs logic but it was too much for now, with my schedule. So now, back to the old one. By the way, the current logic is fairly rudimentary: all members in the cluster/group send a liveness heartbeat to the leader and the leader broadcasting the final list of members to all via hedge‚Äôs broadcast mechanism. CPU usage between the two is fairly similar depending on the sync timeout. I‚Äôve been trying to improve hedge‚Äôs member tracking system as I want to build a distributed in-memory cache within hedge itself. Most of the available ones are Raft-based, and I still haven‚Äôt figured out how to make Raft work in the same deployment configuration.","title":"Attempt to replace hedge‚Äôs member tracking with hashicorp/memberlist"},{"categories":["Tech"],"date":"2023-04-20T00:00:00Z","description":"2023-04-20","keywords":null,"link":"/blog/2023-04-20-alphaus-eng-blog/","tags":["alphaus","blog","engineering","technical"],"text":"Hey there, I wanted to let you know that Alphaus, the startup I work for, has recently launched a new engineering blog. If you‚Äôre interested in checking it out, please go to https://labs.alphaus.cloud/blog/. We will be posting updates about our products and sharing some insights and experiences from a technical point of view. So if that sounds like something you‚Äôd be interested in, feel free to take a look. Thanks for your support!","title":"Alphaus engineering blog launched"},{"categories":["Tech"],"date":"2023-04-13T00:00:00Z","description":"2023-04-13","keywords":null,"link":"/blog/2023-04-13-homebrew-golang-p2/","tags":["homebrew","go","golang","goreleaser","homebrew","ruby"],"text":"For personal reference: This is a followup on a previous post that is a bit more manual. This is a little bit easier. a) Create your own Homebrew tap Create a new GitHub public repository with a prefix homebrew-, i.e. homebrew-tap. This will host all the apps that you want to distribute via your tap. Users will install your apps using the following commands: # No need to include the 'homebrew-' prefix $ brew tap flowerinthenight/tap $ brew install \u003ctoolname\u003e # Or one-liner $ brew install flowerinthenight/tap/\u003ctoolname\u003e b) Let‚Äôs use Github Actions to setup goreleaser First, add a .goreleaser.yml config file to your Go repo. Here‚Äôs an example: The section of note here is the brews: part. You can check goreleaser‚Äôs Quickstart guide for more information. Here‚Äôs an example of a GitHub Action config on how to setup goreleaser to do the release for our tags. See the name: Run goreleaser section. You need to add a personal access token with repo, workflow, write:packages permissions to your repository‚Äôs secrets. In this example, the name used is GH_PAT but you can use other names as well. c) Do a tagged release Tagged releases should now do a deployment to your tap. $ git tag v1.0.0 $ git push --tags","title":"Using Homebrew for distributing Go apps (part 2)"},{"categories":["Tech"],"date":"2023-04-06T00:00:00Z","description":"2023-04-06","keywords":null,"link":"/blog/2023-04-06-restore-default-branch-from-tag/","tags":["git","history"],"text":"Fore self reference: To restore default branch from a tag while preserving history, do: $ git checkout tags/v1.2.3 -b v1.2.3 $ git diff main \u003e /tmp/diff.patch $ git checkout main $ cat /tmp/diff.patch | git apply $ git commit -am \"Rolled back to v1.2.3\" $ git push origin main","title":"Restore default branch from tag while preserving history"},{"categories":["Tech"],"date":"2023-04-01T00:00:00Z","description":"2023-04-01","keywords":null,"link":"/blog/2023-04-01-cto-diaries-on-choosing-tech-stack/","tags":["cto-diaries","cto","diaries","startup"],"text":"When it comes to tech stack selection, in general, I always approach it in terms of tradeoffs. And when it comes to understanding tradeoffs; what you gain vs what you lose, I think experience plays a big part. In my 18+ years of experience building systems, I‚Äôve been bitten enough times that I view most of them with a fair bit of cynicism. I wouldn‚Äôt really say that experience definitely makes me better at it; I still have my own biases and experience doesn‚Äôt really ‚Äòfix‚Äô the illogical side of me being a human being with emotions. It gets in the way at times. Then there‚Äôs the business side of it as well. An example would be to maintain business relations, or to honor partnership agreements, I would need to use (or integrate into) some tech stacks that I wouldn‚Äôt normally choose in the first place. And to add to all of these, I also work in a startup with limited resources so there are other criteria that have a bigger influence on my approach to balancing tradeoffs. And at our phase right now, probably the biggest one is cost. Not just monetary but engineering costs as well: cost of operating and managing these systems. And I must admit that cost considerations is at times so pervasive to me that I would choose a technology purely because I got a significant discount for it even though at the back of my head, I know it‚Äôs going to come back to me later on. So much for experience, huh. The other criteria would be expertise. With our current budget, there is a certain ceiling to the level of engineering expertise I can afford to hire. And so far, ICs or principal level engineers who could handle such decisions for me are still out of my reach. So I still end up making the decisions most of the time. The one or two engineers I have who are fairly knowledgeable about systems and infrastructure are also doing development and technical support at the same time that there‚Äôs just not enough time to do validations. They will have opinions of course based on their readings and experience but at the end of the day, I know that they didn‚Äôt have enough time to validate so I roughly know the level of confidence they have on their opinions. And speaking of opinions, I am more interested in roughly knowing about the caveats, horror stories, real user feedback, etc., that means checking Reddit, Mastodon, Twitter to some extent, engineering blog posts, GitHub issues, HackerNews, etc. I would say that this is almost always a requirement of mine to myself as part of my due diligence. One thing that I enjoy in all of these however is the part where I do a small prototype of the tech in question. It‚Äôs not all the time but when I do, it‚Äôs always a good feeling. It gives me a sense of what the tech is about, what it can do, how it works (sort of), etc. For the core parts of our systems, I think this is pretty much already an imposed personal requirement. With all these requirements in mind, one might say that I will just end up choosing boring, mature, proven, old school tech all the way. For the most part, yes, but I‚Äôm also aware that tech is also part of the hiring strategy. New tech being exciting for most engineers is probably a safe assumption to make so there is also that consideration. With that said though, I‚Äôm very careful with this criteria. Over the years, I‚Äôve come to accept that building systems is engineering/programming over time. Engineers come and go. And tech is evolving at a rapid pace. And so does expertise and opinions evolve with it. Systems are dynamic; they are reshaped and redesigned by the people working on them over time so you will eventually end up with an amalgamation of all sorts of things, even beliefs and convictions. And when all is said and done, the systems running in the company is ultimately my responsibility; I can‚Äôt just put the blame on somebody when things go wrong just because it was their choice and not mine. This leads me to another thing in my criteria: migration. For long-running systems, migrations are a matter of when, not if. I will try to imagine how it‚Äôs going to look like, how painful it will be, and roughly how expensive it will be when the time comes. Then balance it out with us being a startup where the possibility of throwing away code, tech, product(s) even, at any time, along with the engineering behind it, is pretty high. Expanding on the migration bit, this is probably my first time dealing with systems with colossal amounts of data so the considerations for migration is significantly important. This covers our choices of data lakes, high traffic messaging systems, databases, and migrations between these systems. Based on experience, migrations of data-related systems or systems revolving around data are the most protracted and the most painful. Much more so than, say, compute. What about hype, or in GitHub for example, the number of stars a project have? Or in Twitter/Reddit, the chatter? In a way, they are an important consideration as well. Hype results to more early users, which means more feedback, which means more bugs reported, which means more bug fixes. The hype bit though; I‚Äôve been through several hype cycles of tech already that in a way, I can filter them out easily and see through the marketing PR. And I think I‚Äôm not really alone in this. Most of the colleagues I talk to with long-ish experiences under their belt, it‚Äôs true to them as well. So, how do I keep track with all of these information around criteria? Journaling. I take notes on all of them. I‚Äôm still working on the organization part of it though. It was and is a need so I‚Äôm sort of forced into it. But overtime, I think I‚Äôve improved a lot on my documentations (and writing them) and it has grown on me so I don‚Äôt mind it now. Alright, that‚Äôs it for now. See you in the next one.","title":"CTO Diaries #3: On choosing technology stacks"},{"categories":["Tech"],"date":"2023-03-01T00:00:00Z","description":"2023-03-01","keywords":null,"link":"/blog/2023-03-01-authenticate-aws-sdk-v2-external-id/","tags":["golang","aws","sdk","v2","assume","roles","external-id"],"text":"For self reference: Sample code as to how to authenticate aws-sdk-go-v2 using external ids:","title":"Authenticating Go AWS SDK v2 using external id"},{"categories":["Tech"],"date":"2022-10-17T00:00:00Z","description":"2022-10-17","keywords":null,"link":"/blog/2022-10-17-authenticate-aws-sdk-external-id/","tags":["golang","aws","sdk","assume","roles","external-id"],"text":"For self reference: Sample code as to how to authenticate aws-sdk-go using external ids:","title":"Authenticating Go AWS SDK using external id"},{"categories":null,"date":"2022-09-27T00:00:00Z","description":"2022-09-27","keywords":null,"link":"/blog/2022-09-27-cto-diaries-typical-day/","tags":["cto-diaries","cto","diaries","startup"],"text":"Hi. It‚Äôs been a year since my last„ÄåCTO diaries„Äçpost. Never mind the excuses, there‚Äôs a lot. Anyway, so what does my typical, or normal day, look like? Well, I usually start the day with checking the overall health of our systems. We don‚Äôt really have very sophisticated monitoring and alerting systems at this point so this means checking on several areas such as our Kubernetes clusters, databases, and critical services. On top of my head, I think the most common service crashes I noticed so far are OOM-related. I also check our month-to-date infrastructure spending. We use bits of the three (AWS, GCP, and Azure). I don‚Äôt really do this meticulously; more like checking for anomalies. We‚Äôve been hacked several times in the past for mining and these usually show up as cost anomalies. I can also keep an eye on unpredictable usages such as data transfers. Our runway is one of the many things in my list of concerns so I‚Äôd like to keep an eye on runaway costs as much as I can. A huge chunk of the day is spent doing admin tasks. In our current phase, I‚Äôm actually more of an engineering manager than a CTO. I take part in daily standups, backlog grooming, dealing with developer blockers, schedules and priorities, interfacing with technical support, product, and customer success teams. While these are fairly straightforward, I would say that the additional overhead of ‚Äúcontext-switching‚Äù between Japanese and English is surprisingly high, at least for me. The constant worry of whether I was able to communicate what I wanted to communicate, and all the nuances that come with that, is always there. Writing on top of verbal communication always helps. Then 1-on-1‚Äôs. It‚Äôs not daily but it takes a huge amount of my time and energy. It‚Äôs not really that bad overall but it‚Äôs probably the most exhausting, personally. Dealing with people, with different personalities, is always a challenge, both at work and outside. I‚Äôm not a people-person really; my EQ leaves a lot to be desired. But I like to think that I‚Äôm getting good at it though. Another part of my typical day is reading. Anything that I deem beneficial really; technical articles and papers, new developments in technology that are related to our domain, applied psychology ‚Äî interestingly, business and startups, the lot. I would say this is probably the part I enjoy the most. Last is coding, which I usually do during the wee hours of the night. Not core codebase though, mind, but support code such as tooling, CI/CD, and scaling. This is probably out of necessity more than anything. I don‚Äôt really enjoy coding that much anymore, even before I became CTO but we still can‚Äôt afford domain experts at the moment so I contribute as much as I can. I‚Äôll probably write about the other bits like participating in investor meetings, sync-ups with other executives, etc. on a separate post. Alright, that‚Äôs it. See you in the next one.","title":"CTO Diaries #2: Typical day"},{"categories":null,"date":"2022-09-21T00:00:00Z","description":"2022-09-21","keywords":null,"link":"/blog/2022-09-21-spindle-based-distributed-computing-lib/","tags":["gcp","spanner","spindle","distributed-computing","golang"],"text":"This library has been in our production for about a year already and is one of the critical components in our backend. We mainly use it for app-level orchestration between pods. It‚Äôs called hedge and you can find the code here. Maybe it will be useful to anybody out there.","title":"hedge - A simple distributed computing library"},{"categories":null,"date":"2022-01-17T00:00:00Z","description":"2022-01-17","keywords":null,"link":"/blog/2022-01-17-download-latest-github-release-cmdline/","tags":["cmdline","download","github","release"],"text":"For personal reference: Download the latest GitHub Releases asset using common command line tools: # Update the url accordingly. The `uname | awk` subcmds will output 'linux'|'darwin'. $ curl -s https://api.github.com/repos/alphauslabs/bluectl/releases/latest | \\ jq -r \".assets[] | select(.name | contains(\\\"$(uname -s | awk '{print tolower($0)}')\\\")) | .browser_download_url\" | \\ wget -i -","title":"Download the latest Github release using command line"},{"categories":null,"date":"2021-12-31T00:00:00Z","description":"2021-12-31","keywords":null,"link":"/blog/2021-12-31-support-any-grpc-gateway-separate-process/","tags":["protobuf","grpc-gateway","any"],"text":"This is just a quick one. I had some trouble making the Any protobuf type work when running grpc-gateway from a separate process with the gateway throwing an unknown message type error. A quick fix for this is to import the generated pb.go file to your proxy source. Something like: package main import ( ... _ \"github.com/username/pkgwithpbgo\" )","title":"Support the ‚ÄòAny‚Äô protobuf type from an external grpc-gateway process"},{"categories":null,"date":"2021-11-08T00:00:00Z","description":"2021-11-08","keywords":null,"link":"/blog/2021-11-08-alarm-from-cmdline/","tags":["alarm","cmdline","commandline","gnome","linux"],"text":"This is just a quick one. Although I use Gnome Clocks occassionally, most of the time I set an alarm via the commandline. # OS is POP_OS!, using VLC: $ sleep 10m \u0026\u0026 \\ cvlc ~/alarm.mp3 --play-and-exit \u0026\u0026 \\ notify-send 'alarm done'","title":"Setting alarm from commandline"},{"categories":null,"date":"2021-09-28T00:00:00Z","description":"2021-09-28","keywords":null,"link":"/blog/2021-09-28-cto-diaries-intro/","tags":["cto-diaries","cto","diaries","startup"],"text":"So far, I‚Äôve been posting some techy stuff here and there but being a CTO of a relatively small, Japan-based startup for about three years now, I thought I‚Äôd share some of my experiences along the way. But first off, a little bit of context: the company I work for, Alphaus Cloud is fairly small; about 10 engineers scattered across three countries, although at one point, we hit at around 35. It started around 2015, did three product pivots, three rounds of funding under two CEOs. At the moment, we operate within the Cloud FinOps (financial operations) segment of the market. As for myself, I am what you would consider an ‚Äòaccidental CTO‚Äô, a term I came across from CTO Academy, and a term I quite like: i.e. ‚Äúsomeone who arrived in the CTO role ahead of schedule and is grappling with new challenges and very steep learning curve‚Äù. You can call it luck, although that would be relative, depending on one‚Äôs interpretation. Also, Japanese is not my native tongue, although I think I can hold my own in day-to-day conversations, nor is English, but I think I‚Äôm fairly proficient at it. And even though the company is trying to expand its operations outside of Japan, it is still inherently Japanese as far as business operations are concerned: the CEO/COO, sales, customer success teams are Japanese, investors are Japanese, and majority of our clients are also Japanese companies. The CTO role in such a setup is quite unclear. Even though I had some experience leading tech teams in the past, nothing could have prepared me for this role. For one, my blunders are now more costly. My decisions now affect multiple teams, not just the engineering team. I have to undestand the business side of the company. I need to understand company financials. My ‚Äòsoft skills‚Äô now matter more than ever; people, relationships, empathy, politics, the lot: they‚Äôre probably more difficult, and exhausting, than the technology part of my role. With that said though, overall, I‚Äôm enjoying it so far. There‚Äôs a certain charm to it that I can‚Äôt really explain; I thought I wouldn‚Äôt like it at first, me being an introvert and all, but the challenges are instructional and character-building enough that I welcome, sort of, the stress associated with it. My goal in this series is to share some of my experiences from this side of the world in the hopes that it will help others gain some perspective. Mind you, this is the internet, and these are just my opinions, take them with a pinch of salt. And finally, I‚Äôm still not sure how to make this series more accessible from the blog. The ‚Äôtags‚Äô section is probably the faster way to search for it for now. Maybe I‚Äôll provide a dedicated page in the future, who knows, we‚Äôll see. Alright, that‚Äôs it. See you in the next one.","title":"CTO Diaries #1: Introduction"},{"categories":null,"date":"2021-07-31T00:00:00Z","description":"2021-07-31","keywords":null,"link":"/blog/2021-07-31-extracting-grpc-funcs-to-list/","tags":["golang","grpc","grpc-gateway","cmdline"],"text":"This might be hacky and there might be a proper way to do this but recently, I needed to generate all the functions‚Äô gRPC-generated full names from our protobuf definitions. This is part of our RBAC module that needs to filter gRPC function calls. I got the list from our generated Go client using the following command(s): # Main command: $ grep -o -R -i -E '\"/blueapi\\..*\"' . | awk -F':' '{gsub(/\"/, \"\", $2); print \"-\", substr($2, 2);}' | sort | uniq # Actual commands; save as yaml: $ echo \"functions:\" \u003e /tmp/funcs.yaml $ grep -o -R -i -E '\"/blueapi\\..*\"' . | awk -F':' '{gsub(/\"/, \"\", $2); print \"-\", substr($2, 2);}' | \\ sort | uniq \u003e\u003e /tmp/funcs.yaml The final list is uploaded to this repository.","title":"Extract gRPC-generated functions to a list"},{"categories":null,"date":"2021-06-30T00:00:00Z","description":"2021-06-30","keywords":null,"link":"/blog/2021-06-30-hacky-way-update-go-modules/","tags":["golang","modules","update"],"text":"The only options for updating modules that I‚Äôm aware so far are a) go get -u all, and b) specific modules, i.e. go get -u domain.com/module[@v1.2.3]. For problematic ones, my only option is b), which is a bit time consuming. There must be some other way out there that I‚Äôm not aware of but at the moment, I use this simple command: $ cat go.mod | grep -i 'github' | grep -i -v 'indirect' | awk '{print $1}' \u003e update; \\ while read -r v; do go get -u $v; done \u003c update; rm update","title":"A hacky way to update problematic Go modules"},{"categories":null,"date":"2021-05-31T00:00:00Z","description":"2021-05-31","keywords":null,"link":"/blog/2021-05-31-switching-different-go-versions/","tags":["golang","versions","switch"],"text":"I use this handy little script to switch between different Go (golang) versions: #!/bin/bash $1 version \u0026\u003e/dev/null if [ $? -eq 0 ]; then ln -sf $GOPATH/bin/$1 $HOME/.local/bin/go go version exit 0 fi go get golang.org/dl/$1 \u0026\u0026 $1 download ln -sf $GOPATH/bin/$1 $HOME/.local/bin/go go version Saving this as an executable script goset, I could now switch to different versions like so: $ goset go1.16.4 go version go1.16.4 linux/amd64","title":"Switching to different Go versions"},{"categories":null,"date":"2021-04-30T00:00:00Z","description":"2021-04-30","keywords":null,"link":"/blog/2021-04-30-authenticate-aws-sdk-golang-v2/","tags":["golang","aws","sdk","v2","assume","roles"],"text":"For self reference: To authenticate the Golang AWS SDK v2 using assume roles, refer to the following code snippets:","title":"Authenticating Go AWS SDK v2 using assume roles"},{"categories":null,"date":"2021-03-27T00:00:00Z","description":"2021-03-27","keywords":null,"link":"/blog/2021-03-27-minimalist-conky/","tags":["conky","minimalis"],"text":"I am a fan of the Minimalis conky config for quite some time now. I‚Äôve tweaked it a little bit due to the original config not displaying some of the information. Mind you, my system is Linux-based (Pop!_OS) using wired connection only, no WIFI, so I‚Äôve removed the WIFI section and replaced it with the wired one. Here‚Äôs a screenshot of it from my desktop. The full configuration can be found here, in case you‚Äôre interested. Changes as far as I can remember are the addition of additional CPU cores, both CPU and GPU temperatures, processes, and network.","title":"Updated config for Minimalis conky"},{"categories":null,"date":"2021-02-21T00:00:00Z","description":"2021-02-21","keywords":null,"link":"/blog/2021-02-21-update-spacemacs-cmdline/","tags":["emacs","spacemacs"],"text":"Reference: Ever since I‚Äôve started using Spacemacs on Linux, I‚Äôve been using this simple script as a command alias to update Spacemacs using the command line. alias upe='git -C ~/.emacs.d/ pull \u0026\u0026 \\ emacs --batch -l ~/.emacs.d/init.el \\ --eval=\"(configuration-layer/update-packages t)\" 2\u003e\u00261 | \\ tee /tmp/emacs-update \u0026\u0026 \\ grep -i -E \"Found.*to.*update.*\" /tmp/emacs-update \u0026\u0026 \\ emacs' I use the develop branch of Spacemacs, thus the update using git. Then it will check if there are package updates and if so, it will relaunch Spacemacs to actually install the updates.","title":"Update Spacemacs from command line"},{"categories":null,"date":"2021-01-30T00:00:00Z","description":"2021-01-30","keywords":null,"link":"/blog/2021-01-30-vim-emacs/","tags":["vim","emacs","spacemacs"],"text":"It‚Äôs been a year since I‚Äôve transitioned, sort of, from Vim to Emacs, so I thought I‚Äôd write something about it. Vim has been my primary editor of choice for many years. I think I started Vim around 2003 on a RedHat box writing x86 assembly. Ever since, throughout my working carrier, whether it‚Äôs on Windows, OSX, or Linux, it‚Äôs been always Vim for me. Yes, even on Windows where the experience is really not that good, I still use Vim. My config now is reasonably compact after all the changes it went through with the plugins that I‚Äôve used all these years. But around January of 2020, I stumbled upon a YouTube video about Org Mode. When I saw it, I thought it was fantastic. My wife has been doing bullet journaling for a while now so I‚Äôm a bit familiar with it and I thought it‚Äôs pretty much a bullet journaling plugin in an editor. I gave it a try. I tried both Spacemacs and Doom Emacs and eventually settled with Spacemacs. Not sure why. Both are brilliant. My leader key in Vim is Space so the transition was not that difficult. Now, Org Mode has become an essential tool for me in my work. I don‚Äôt really code that much anymore so I spend more time in Org Mode now than in code. After a year -ish of Spacemacs usage, overall, my experience has been enjoyable. Mind you, I‚Äôve only used it in Linux as my main system is Pop!_OS. Some minor annoyances. First is the startup time. I never really thought much about it in Vim. You could say that I‚Äôve been spoiled with Vim‚Äôs startup times that Spacemacs‚Äô startup delay was a hard pill to swallow. It still is. At this time of writing, I‚Äôm on Emacs 26.3. I tried 27.1 which is supposed to be faster due to its native support for JSON parsing, but, while faster overall, still noticeably and annoyingly slow. Still, once I open Spacemacs, I rarely close it, or I‚Äôve learned not to close it the way I use Vim so I‚Äôd say I can live with it. Second is the s shortcut key. It‚Äôs not really against Spacemacs or anything but after a year of use, I still stumble with it more times than I like. You see, in Vim, my goto ‚Äòreplace‚Äô shortcut is s by highlighting a word, or phrase, or a line, then pressing s to delete it while changing to edit mode at the same time. In Spacemacs, s is surround, not delete+edit. It‚Äôs probably going to take me more years to master this especially that I still use Vim from time to time. Now, I still use Vim for quick edits, or, if I feel like it. It still has a special place in my heart. But for now, I look forward to more years of Emacs use.","title":"From Vim to Emacs"},{"categories":null,"date":"2020-12-26T00:00:00Z","description":"2020-12-26","keywords":null,"link":"/blog/2020-12-26-spanner-distributed-lock/","tags":["gcp","spanner","distributed-lock","distributed-locking","golang"],"text":"I uploaded a yet another distributed locking library, this time, based on Cloud Spanner and TrueTime technology. It‚Äôs called spindle and you can find the code here. Maybe it will be useful to anybody out there.","title":"A Spanner-based distributed locking library"},{"categories":null,"date":"2020-10-31T00:00:00Z","description":"2020-10-31","keywords":null,"link":"/blog/2020-10-31-oops/","tags":["k8s","kubernetes","testing","api"],"text":"I uploaded an automation-friendly, highly-scalable, and scriptable API/generic testing tool built to run on Kubernetes. It‚Äôs called oops and you can find the code here. Maybe it will be useful to anybody out there.","title":"An automation-friendly, highly-scalable, and scriptable testing tool"},{"categories":null,"date":"2020-09-11T00:00:00Z","description":"2020-09-11","keywords":null,"link":"/blog/2020-09-11-autobackup-spanner-k8s/","tags":["spanner","gcp","k8s","cronjob","backup"],"text":"As of this writing, GCP doesn‚Äôt have an option to create Spanner backups automatically. This could be available when you‚Äôre reading this in the future. At the moment, however, if you‚Äôre using Kubernetes, you can utilize CronJob to do a scheduled backup. Here‚Äôs a sample CronJob deployment that uses gcloud to create the backups. First, you need to create a service account that has permissions to create Spanner backups. Once you have downloaded the JSON file for the service account, store it as a Kubernetes Secret. $ kubectl create secret generic spannerbackup-keyfile --from-file svcacct.json Make sure to update some of the information below as required, such as, name of the backup, instance name, database name, expiration date, etc. The example below uses the backup name autobackup-yyyymmddthhmmssutc format with a 3-day backup expiration. apiVersion: batch/v1beta1 kind: CronJob metadata: name: spannerbackup spec: concurrencyPolicy: Forbid # Run every day @ 2:30am JST (below is UTC) schedule: \"30 17 * * *\" jobTemplate: spec: template: spec: containers: - name: spannerbackup image: google/cloud-sdk:307.0.0-slim command: [\"/bin/bash\"] args: [\"-c\", \"NAME=autobackup-$(date +%Y%m%dT%H%M%S%Z | awk '{print tolower($0)}'); EXP=$(date -u -d '+3 day' +%FT%TZ); gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS \u0026\u0026 gcloud spanner backups create $NAME --instance=\u003cinstancename\u003e --database=\u003cdbname\u003e --expiration-date=$EXP --async\"] env: - name: GET_HOSTS_FROM value: dns - name: GOOGLE_APPLICATION_CREDENTIALS value: /etc/spannerbackup/svcacct.json volumeMounts: - name: keyfile mountPath: \"/etc/spannerbackup\" readOnly: true restartPolicy: OnFailure volumes: - name: keyfile secret: secretName: spannerbackup-keyfile Finally, deploy to k8s. # Assuming above file is saved as `backup.yaml` $ kubectl create -f backup.yaml","title":"Automate Spanner backup using Kubernetes CronJob"},{"categories":null,"date":"2020-08-31T00:00:00Z","description":"2020-08-31","keywords":null,"link":"/blog/2020-08-31-dysync/","tags":["aws","dynamodb","sync","golang"],"text":"I uploaded a tool that can sync DynamoDB tables between accounts. It‚Äôs called dysync and you can find the code here. Maybe it will be useful to anybody out there.","title":"A tool for synching DynamoDB tables between accounts"},{"categories":null,"date":"2020-07-08T00:00:00Z","description":"2020-07-08","keywords":null,"link":"/blog/2020-07-08-package-distributed-locks/","tags":["redis","redislock","k8s","k8slock","leaselock","distributed-locks"],"text":"I‚Äôve uploaded a package for distributed locks. It‚Äôs called dlock. At the moment, it supports using Kubernetes‚Äô LeaseLock object and Redis. You might find it useful.","title":"dlock - package for distributed locks"},{"categories":null,"date":"2020-06-17T00:00:00Z","description":"2020-06-17","keywords":null,"link":"/blog/2020-06-17-string-split-awk/","tags":["awk","commandline","string","split"],"text":"For personal reference: Lately, I‚Äôve been doing loads of CSV processing on the terminal with lots of string replacing/splitting. There are probably better (shorter) ways to do this but I‚Äôve been leaning on awk a lot recently. Some examples. # One-line separator replace: $ echo \"1,2,3,4,5\" | awk '{gsub(/,/,\" \");print}' 1 2 3 4 5 # Then I can use it with something like: for v in $(echo \"1,2,3,4,5\" | awk '{gsub(/,/,\" \");print}'); do echo \"val=$v\"; done val=1 val=2 val=3 val=4 val=5 # Another separator: $ echo \"1|2|3|4|5\" | awk '{gsub(/\\|/,\",\");print}' 1,2,3,4,5 # Join multiple lines with a separator... # Ref: https://www.baeldung.com/linux/join-multiple-lines $ cat file.txt one two three four five five # ...while removing duplicates: $ cat file.txt | uniq | awk -v d=\",\" '{s=(NR==1?s:s d)$0}END{print s}' one,two,three,four,five","title":"Simple string manipulations (splits/replaces) with awk"},{"categories":null,"date":"2020-05-13T00:00:00Z","description":"2020-05-13","keywords":null,"link":"/blog/2020-05-13-lsdy-dynamodb-query/","tags":["aws","dynamodb","query","golang"],"text":"I uploaded a tool that can query DynamoDB tables. It‚Äôs called lsdy and you can find the code here. Maybe it will be useful to anybody out there.","title":"A tool for querying DynamoDB"},{"categories":null,"date":"2020-04-22T00:00:00Z","description":"2020-04-22","keywords":null,"link":"/blog/2020-04-22-open-manpages-vim/","tags":["man","manual","vim"],"text":"Most of the time when I open a man page for a certain command or tool, I use Vim: # Open man page for the 'dd' command: $ man dd | col -b | vim -MR - It‚Äôs actually a handful to type but since I use zsh + fzf, it‚Äôs not really a big deal as recalling the command is pretty straightforward. But yesterday, I came across this nifty little plugin called vim-manpager. I tried adding it to my vimrc, added the environment variable below to my zshrc: export MANPAGER=\"vim -c MANPAGER -\" And now, I can open all man pages by calling the man command directly. I can also open man pages directly from within Vim. Brilliant!","title":"Open man pages in Vim"},{"categories":null,"date":"2020-03-25T00:00:00Z","description":"2020-03-25","keywords":null,"link":"/blog/2020-03-25-vimgo-gocode-gopls/","tags":["vim","vim-go","gocode","autocomplete","gopls"],"text":"Update vim-go has already transitioned to gopls as its default completion engine. I just noticed that my Vim is still using gocode even though I updated to the latest version of vim-go. Commenting/removing the line Plugin 'stamblerre/gocode', {'rtp': 'vim/'} from my .vimrc did the trick for me. Hope this helps. Original post here.","title":"Follow-up on autocompletion with gocode and vim-go in Go1.11, transition to gopls"},{"categories":null,"date":"2020-03-25T00:00:00Z","description":"2020-03-25","keywords":null,"link":"/blog/2020-03-25-torguard-wireguard-ubuntu/","tags":["torguard","wireguard","ubuntu","linux"],"text":"As of this writing, Torguard‚Äôs desktop app still doesn‚Äôt support Wireguard out of the box. I‚Äôm using Ubuntu 19.10 for testing. If you haven‚Äôt installed Wireguard yet, you can refer to the installation instructions here. In my case, it was: $ sudo apt install wireguard Next, you need to login to your Torguard client area, go to Tools ‚Äì\u003e Enable Wireguard Access. Select a location (in my case, I chose Asia-Singapore2) then click ‚ÄòEnable Wireguard‚Äô. You will be presented with an option to download the generated config file (in my case Asia-Singapore2.conf). Download it and save it to /etc/wireguard/. To enable the VPN connection, run: # The last bit should correspond to the filename of your config file without the '.conf', # in my case 'Asia-Singapore2' $ sudo wg-quick up Asia-Singapore2 # Confirm with: $ sudo wg # If no issues, you can check your public IP with: $ curl ipecho.net/plain; echo Finally, to disconnect, run: $ sudo wg-quick down Asia-Singapore2 That‚Äôs it.","title":"Connecting to Torguard VPN using Wireguard from Ubuntu"},{"categories":null,"date":"2020-03-18T00:00:00Z","description":"2020-03-18","keywords":null,"link":"/blog/2020-03-18-glog-gotest/","tags":["golang","glog","test"],"text":"If you‚Äôre using glog in your Go codes, you can output those when running go test ... by using the --args parameter: $ go test -v ./... -count=1 -cover -race -mod=vendor --args --logtostderr --v=1","title":"Output glog from go test"},{"categories":null,"date":"2020-02-27T00:00:00Z","description":"2020-02-27","keywords":null,"link":"/blog/2020-02-27-cobra-tabwriter/","tags":["cobra","tabwriter","golang"],"text":"If you‚Äôve been following the blog, you know that I‚Äôm a fan of cobra as a CLI library. Let me share how I use tabwriter to compliment cobra‚Äôs autogenerated help information. For reference, you can check this post as well. The following code is a copy (not an exact copy) of one of the tools that I use at work. It will look something like this. Trigger a manual run, among other tools. Format for the --input flag: payer:\u003cid\u003e[/link1,link2,...] run manual calculation at payer level, optional link acct list is for log filtering msp:\u003cid\u003e run manual calculation at MSP level link:\u003cid\u003e run manual calculation at link acct level detectri:\u003cmsp|payer\u003e[:id] run RI detection manually detectsp:\u003cmsp|payer\u003e[:id] run SavingsPlan detection manually curlinks:\u003cmsp|payer\u003e[:id] detect actual customers from CUR, can use --detect-moved-payer flag fees:\u003cid\u003e detect fees for the input link id detecttags:\u003cid\u003e detect tags for the input payer id Notes: * When triggering CUR (using trigger: input), it's recommended to use the --use-sns flag. Usage: linkbatchd manualrun [flags] Flags: --input string see subcommand description for more details --date string date to trigger in UTC, fmt: yyyy-mm-dd, yyyy-mm-dd,yyyy-mm-dd for date range (from,to) --invoice-type string type of invoice to calculate: 'account' or 'cross_tag', used in trigger: input","title":"Using tabwriter to improve on cobra‚Äôs help information"},{"categories":null,"date":"2020-01-27T00:00:00Z","description":"2020-01-27","keywords":null,"link":"/blog/2020-01-27-kubepfm-update/","tags":["kubepfm","kubectl","k8s","kubernetes","port-forward"],"text":"Updated 2020/01/27: Support for forwarding to deployments and services Original post: I recently uploaded a tool to GitHub that wraps kubectl port-forward command and supports port-forwarding to multiple pods. It‚Äôs called kubepfm. I use this tool heavily in my development work. You can check out the code here. You might find this useful.","title":"Update to kubepfm, a kubectl port-forward wrapper for multiple pods"},{"categories":null,"date":"2019-11-28T00:00:00Z","description":"2019-11-28","keywords":null,"link":"/blog/2019-11-28-gcp-backendconfig-k8s/","tags":["k8s","timeout","gcp","ingress"],"text":"This is related to a previous post about Kubernetes services. This time, it‚Äôs about extending the timeout of an Ingress. We had a situation where we had to download a huge file from one of our exposed services. The download takes about two minutes to complete. This didn‚Äôt really worked out since by default, GCP load balancers that are associated with k8s Ingresses have a timeout value of 30s. For a time, we just did manual updates by going to the GCP k8s Services and Ingress console, opening the backend service under the Ingress, and editing the Timeout section to the desired seconds. But since we do have a cluster blue/green deployment, we had to do this every time we recreate our clusters. Enter BackendConfig custom resource. With this, we can associate a BackendConfig resource to the service in question using GCP-specific annotations. Reusing the reverse proxy YAML from this post, we add a BackendConfig resource. apiVersion: cloud.google.com/v1beta1 kind: BackendConfig metadata: name: serviceproxy-backendconfig spec: timeoutSec: 7200 connectionDraining: drainingTimeoutSec: 60 --- apiVersion: v1 kind: Service metadata: name: serviceproxy annotations: beta.cloud.google.com/backend-config: '{\"ports\": {\"80\":\"serviceproxy-backendconfig\"}}' spec: type: NodePort ports: - name: http protocol: TCP port: 80 targetPort: 80 selector: app: serviceproxy Deploying this will set the timeout of the service to two hours.","title":"Extending the timeout of a Kubernetes service in GCP"},{"categories":null,"date":"2019-10-29T00:00:00Z","description":"2019-10-29","keywords":null,"link":"/blog/2019-10-29-stderr-tee/","tags":["golang","tee","stderr","stdout"],"text":"Some golang-based tools I‚Äôve used (or even written) use the builtin log package that output logs to stderr by default. I also use tee for piping console outputs to file for later viewing. This is the command I generally use: # Example tool: $ sometool --flag1 --flag2 2\u003e\u00261 | tee out.txt # 2\u003e\u00261 \u003c-- redirect stderr to stdout # tee \u003c-- pipe the console output to out.txt while retaining the actual console logs during command execution","title":"Output golang cmdline tools to stdout and file using tee"},{"categories":null,"date":"2019-09-30T00:00:00Z","description":"2019-09-30","keywords":null,"link":"/blog/2019-09-30-stern-grep/","tags":["stern","grep","k8s","logs"],"text":"I‚Äôve been using stern as my goto log viewer for Kubernetes. It supports multiple pods and other convenient functions on top of kubectl logs. And of course, grepping goes hand in hand with log viewing, doesn‚Äôt it? To combine grep with stern, I use the following commands: # For OSX: # stern \u003csome-prod-prefix\u003e | grep -i --line-buffered -E '\u003cextended-regex\u003e' $ stern user | grep -i --line-buffered -E 'failed' # For Linux (Ubuntu specifically): # stern \u003csome-prod-prefix\u003e | grep -i -E '\u003cextended-regex\u003e' $ stern user -s 1h | grep -i -E 'error'","title":"Using stern together with grep"},{"categories":null,"date":"2019-08-14T00:00:00Z","description":"2019-08-14","keywords":null,"link":"/blog/2019-08-14-ssh-tunnel-home-router/","tags":["ssh","tunnel","home","router"],"text":"For personal reference: My current setup: Router has a static IP provided by my ISP Router web server is accessible from 192.168.1.1 (home network) SSH server box in my home network with a static IP of 192.168.1.130 (home network) Router port forwarding is setup to forward router SSH port to my SSH server box (192.168.1.130:22) SSH server box is configured to only accept public/private key authentication; password disabled Reason for access: I want to be able to access my router web server from outside to configure some settings on the fly, like VPN settings, etc. How: Run the following command: # -L form: -L local-port:target:target-port # 'user' = SSH username $ ssh -i /path/to/ssh/key -L 8080:192.168.1.1:80 user@router-static-ip Open localhost:8080 to access the router web server. That‚Äôs it!","title":"Using SSH tunnelling to access home router"},{"categories":null,"date":"2019-07-30T00:00:00Z","description":"2019-07-30","keywords":null,"link":"/blog/2019-07-30-homebrew-golang/","tags":["homebrew","go","golang","ruby"],"text":"For personal reference: a) Create your own homebrew tap Create a new GitHub public repository with a prefix homebrew-, i.e. homebrew-tap. This will house all the apps that you want to distribute via your tap. Users will install your apps using the following commands: # No need to include the 'homebrew-' prefix $ brew tap flowerinthenight/tap $ brew install \u003ctoolname\u003e The toolname part will correspond to the filename inside your repository tap. If your repository has an entry with a filename of toolx.rb, it can be installed using the following commands: $ brew tap flowerinthenight/tap $ brew install toolx Here‚Äôs an example of a formula for a golang app: class Kubepfm \u003c Formula desc \"A simple wrapper to kubectl port-forward for multiple pods.\" homepage \"https://github.com/flowerinthenight/kubepfm\" url \"https://github.com/flowerinthenight/kubepfm/archive/v0.0.3.tar.gz\" sha256 \"7852a5500f3e35a47b57d138c756de5641bc3c48bf7e329d2724c1107ccb1207\" depends_on \"go\" def install ENV[\"GOPATH\"] = buildpath ENV[\"GO111MODULE\"] = \"on\" ENV[\"GOFLAGS\"] = \"-mod=vendor\" ENV[\"PATH\"] = \"#{ENV[\"PATH\"]}:#{buildpath}/bin\" (buildpath/\"src/github.com/flowerinthenight/kubepfm\").install buildpath.children cd \"src/github.com/flowerinthenight/kubepfm\" do system \"go\", \"build\", \"-o\", bin/\"kubepfm\", \".\" end end test do assert_match /Simple port-forward wrapper tool for multiple pods/, shell_output(\"#{bin}/kubepfm -h\", 0) end end You can check out https://github.com/flowerinthenight/homebrew-tap for reference. The url part is the path of the source tar.gz of your source files. You can create this by using tagged releases in GitHub. You can generate the sha256 part by running the shasum (OSX) or sha256sum (Linux) tool against your tar.gz file. b) Updating your formula If you have a new version of your tool, first, create a new tag or release. Download the new tar.gz file of the new release, run the sha256sum/shasum tool against it, and update the .rb file in your tap repository. Example: $ git clone https://github.com/flowerinthenight/kubepfm $ cd kubepfm/ $ git tag v1.0.1 # our new tag $ git push --tags # push tags to remote $ wget https://github.com/flowerinthenight/kubepfm/archive/v1.0.1.tar.gz # OSX $ shasum -a 256 v1.0.1.tar.gz 7852a5500f3e35a47b57d138c756de5641bc3c48bf7e329d2724c1107ccb1207 v1.0.1.tar.gz # Linux $ sha256sum v1.0.1.tar.gz 7852a5500f3e35a47b57d138c756de5641bc3c48bf7e329d2724c1107ccb1207 v1.0.1.tar.gz Finally, update the url and sha256 part of your .rb file. Users will now be able to update their copies: $ brew upgrade kubepfm","title":"Using Homebrew for distributing Go (golang) apps"},{"categories":null,"date":"2019-06-05T00:00:00Z","description":"2019-06-05","keywords":null,"link":"/blog/2019-06-05-master-election-using-kettle/","tags":["redis","distributed-locking","master","worker"],"text":"I uploaded a simple library that abstracts the use of distributed locking to elect a master among group of workers at a specified time interval. It‚Äôs called kettle. You can find the source code here. We‚Äôve been using this library mostly on these two recurring use case patterns: Consuming a dynamodbstreams endpoint across multiple containers running in Kubernetes. In this case, the master tracks the dynamic addition/removal of shards of a specific endpoint and distributes those shards to the other worker containers. Distributing processing to multiple containers from a list of work items. In this case, we have a table of items where each item needs some processing applied to it. We scale out the number of containers to something directly proportional to the number of items on the list. Instead of all containers querying the same table simultaneously, the master will do the querying, publish each item to a pubsub topic, and workers will do the processing from the pubsub items.","title":"Using kettle library for master election using distributed locking"},{"categories":null,"date":"2019-05-26T00:00:00Z","description":"2019-05-26","keywords":null,"link":"/blog/2019-05-26-cors-nginx-k8s/","tags":["nginx","proxy","cors","Kubernetes"],"text":"At work, for a couple of months now, we‚Äôve been using Ambassador as our main API gateway to our k8s services. We also have our own authorization service that uses Ambassador‚Äôs AuthService mechanism. Recently, we‚Äôve had services that needed CORS support and although Ambassador has features that support the enabling of CORS, we had to update our authorization service to handle CORS-related requests. Instead of doing this, we tried adding the CORS support at the proxy level (nginx). I‚Äôve wrote about this topic here and here. In the example below, the CORS support is added under the location /svc2/. --- apiVersion: v1 kind: ConfigMap metadata: name: serviceproxy-conf data: serviceproxy.conf: | server { listen 80; server_name development.mobingi.com; resolver kube-dns.kube-system.svc.cluster.local valid=10s; location ~ ^/svc1/(.*)$ { auth_basic \"mobingi\"; auth_basic_user_file /etc/serviceproxy/htpasswd; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $host; rewrite ^/svc1/(.*)$ /$1 break; proxy_pass \"http://svc1.default.svc.cluster.local\"; proxy_http_version 1.1; } location ~ ^/svc2/(.*)$ { # Ref: https://enable-cors.org/server_nginx.html if ($request_method = 'OPTIONS') { add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Methods' 'POST, OPTIONS'; # Custom headers and headers various browsers *should* be OK with but aren't. add_header 'Access-Control-Allow-Headers' 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization'; # Tell client that this pre-flight info is valid for 20 days. add_header 'Access-Control-Max-Age' 1728000; add_header 'Content-Type' 'text/plain; charset=utf-8'; add_header 'Content-Length' 0; return 204; } proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $host; rewrite ^/svc2/(.*)$ /$1 break; proxy_pass \"http://svc2.default.svc.cluster.local\"; proxy_http_version 1.1; } # root health check requirement in GKE ingress location / { return 200 'healthy\\n'; } } {..redacted..} With that said, be aware of the potential problems of using if in nginx.","title":"Adding CORS support to nginx proxy in Kubernetes"},{"categories":null,"date":"2019-04-28T00:00:00Z","description":"2019-04-28","keywords":null,"link":"/blog/2019-04-28-drv2605-umdf-haptic-driver/","tags":["drv2605","umdf","haptic","driver","windows"],"text":"I uploaded an ancient prototype driver code I used ages ago in testing the DRV2605 haptic device from Texas Instruments. I can‚Äôt figure out (and I also don‚Äôt have time either) how to make this thing build using the recent versions of Visual Studio. Nonetheless, I‚Äôm putting this out there as reference to somebody doing something similar to this project. Source code is here.","title":"UMDF driver code for DRV2605 haptic device"},{"categories":null,"date":"2019-03-13T00:00:00Z","description":"2019-03-13","keywords":null,"link":"/blog/2019-03-13-ccfltr-updates/","tags":["wdm","filter-driver","camera","driver","windows"],"text":"Updated 2019/03/13: A pretty good writeup that explains the internals of writing a camera filter driver in Windows can be found here. I‚Äôm putting this information out as this repo is one of the references used in the writeup. Original post: This post is a bit of a departure from my usual golang/cloud-related ramblings. I posted an open-source camera class filter driver for Windows ages ago hoping that it would help someone working on a similar project. If you are familiar with this type of driver, you probably know that it‚Äôs not that straightforward to write mainly due to it being generally undocumented. A lot of reverse engineering has been done to write this driver. Anyway, recently, someone pointed out to me that it‚Äôs been discussed in this forum post. So if you‚Äôre working on a similar project, give it a whirl, and if you find some issues, I would appreciate it if you could submit a PR for fixes. Thanks.","title":"Updates to Camera Class Filter Driver for Windows"},{"categories":null,"date":"2019-03-09T00:00:00Z","description":"2019-03-09","keywords":null,"link":"/blog/2019-03-09-athena2csv/","tags":["athena2csv","athena","csv","csv","aws","golang"],"text":"I recently uploaded a tool to GitHub that downloads AWS Athena query results as CSV. It‚Äôs called athena2csv. You can check out the code here. You might find this useful.","title":"Download AWS Athena query results as CSV"},{"categories":null,"date":"2019-02-20T00:00:00Z","description":"2019-02-20","keywords":null,"link":"/blog/2019-02-20-kubepfm-update/","tags":["kubepfm","kubectl","k8s","kubernetes","port-forward"],"text":"Updated 2019/02/20: Support for namespaces https://github.com/flowerinthenight/kubepfm/issues/1 Improved handling of input patterns Original post: I recently uploaded a tool to GitHub that wraps kubectl port-forward command and supports port-forwarding to multiple pods. It‚Äôs called kubepfm. I use this tool heavily in my development work. You can check out the code here. You might find this useful.","title":"Update to kubepfm, a kubectl port-forward wrapper for multiple pods"},{"categories":null,"date":"2019-02-05T00:00:00Z","description":"2019-02-05","keywords":null,"link":"/blog/2019-02-05-golang-cobra-klog/","tags":["golang","klog","cobra"],"text":"This post is somehow related to a previous article about using glog together with cobra. This time, we will be using klog which is a Kubernetes fork of glog. # run the -h command $ ./cobraklog -h Usage of ./cobraklog: -alsologtostderr log to standard error as well as files -log_backtrace_at value when logging hits line file:N, emit a stack trace -log_dir string If non-empty, write log files in this directory -log_file string If non-empty, use this log file -logtostderr log to standard error instead of files -skip_headers If true, avoid header prefixes in the log messages -stderrthreshold value logs at or above this threshold go to stderr (default 2) -v value log level for V logs -vmodule value comma-separated list of pattern=N settings for file-filtered logging # run cobra's `help` command $ ./cobraklog help Use klog together with cobra. Usage: cobraklog [command] Available Commands: help Help about any command run run command Flags: --alsologtostderr log to standard error as well as files --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory --log_file string If non-empty, use this log file --logtostderr log to standard error instead of files --skip_headers If true, avoid header prefixes in the log messages --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging -h, --help help for cobraklog Use \"cobraklog [command] --help\" for more information about a command. # run `help` on our subcommand `run` $ ./cobraklog help run Run command. Usage: cobraklog run [flags] Flags: --str string string to print (default \"hello world\") -h, --help help for run Global Flags: --alsologtostderr log to standard error as well as files --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory --log_file string If non-empty, use this log file --logtostderr log to standard error instead of files --skip_headers If true, avoid header prefixes in the log messages --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging # run the `run` subcommand $ ./cobraklog run --logtostderr I0205 16:37:39.110849 13672 main.go:41] echo=hello world $ ./cobraklog run --logtostderr --str \"hello alien world\" I0205 16:39:37.212187 13685 main.go:41] echo=hello alien world","title":"Using k8s.io/klog together with cobra in golang"},{"categories":null,"date":"2019-01-31T00:00:00Z","description":"2019-01-31","keywords":null,"link":"/blog/2019-01-31-nginx-basicauth-k8s/","tags":["nginx","Kubernetes","basic-auth"],"text":"This post is somewhat related to a previous post about accessing k8s services using nginx reverse proxy. Let‚Äôs try to add a simple basic authentication to these services at the proxy level. Now, this may come in handy in non-production environments but at the very least, make sure that you are doing this over HTTPS as basic authentication credentials are not encrypted. We will be using the htpasswd tool to generate our passwords. In Ubuntu, you can install this using the following command: $ sudo apt-get install apache2-utils Let‚Äôs generate our password file: $ htpasswd -c passfile user1 New password: Re-type new password: Adding password for user user1 $ cat passfile user1:$apr1$c/7lb2VS$SQ9pPJ8XfNpPH.jmnHRsE0 Let‚Äôs add a config map to our previous YAML file and enable basic authentication to svc1 only: apiVersion: v1 kind: ConfigMap metadata: name: basicauth data: htpasswd: | # generate: $ htpasswd -c {file} username (then input password) user1:$apr1$c/7lb2VS$SQ9pPJ8XfNpPH.jmnHRsE0 --- apiVersion: v1 kind: ConfigMap metadata: name: serviceproxy-conf data: serviceproxy.conf: | server { listen 80; server_name development.mobingi.com; resolver kube-dns.kube-system.svc.cluster.local valid=10s; location ~ ^/svc1/(.*)$ { auth_basic \"mobingi\"; auth_basic_user_file /etc/serviceproxy/htpasswd; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $host; rewrite ^/svc1/(.*)$ /$1 break; proxy_pass \"http://svc1.default.svc.cluster.local\"; proxy_http_version 1.1; } location ~ ^/svc2/(.*)$ { proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $host; rewrite ^/svc2/(.*)$ /$1 break; proxy_pass \"http://svc2.default.svc.cluster.local\"; proxy_http_version 1.1; } # root health check requirement in GKE ingress location / { return 200 'healthy\\n'; } } --- apiVersion: apps/v1 kind: Deployment metadata: name: serviceproxy spec: replicas: 1 revisionHistoryLimit: 3 selector: matchLabels: app: serviceproxy template: metadata: labels: app: serviceproxy spec: containers: - name: nginx image: nginx:1.13 ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: /etc/nginx/conf.d/ - name: htpasswd mountPath: /etc/serviceproxy/ volumes: - name: config-volume configMap: name: serviceproxy-conf items: - key: serviceproxy.conf path: serviceproxy.conf - name: htpasswd configMap: name: basicauth items: - key: htpasswd path: htpasswd --- apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: serviceproxy-hpa ... You should now be able to access svc1 using your username:password. $ curl -u user1:password https://development.mobingi.com/svc1/some-endpoint That‚Äôs it!","title":"Using nginx basic authentication in Kubernetes"},{"categories":null,"date":"2018-12-27T00:00:00Z","description":"2018-12-27","keywords":null,"link":"/blog/2018-12-27-mochi-k8s-repo/","tags":["go","k8s","kubernetes","repo","mobingi"],"text":"I posted an article for @mobingi about its Kubernetes clusters repo. Please check it out. Thanks.","title":"Mobingi‚Äôs Kubernetes clusters repo"},{"categories":null,"date":"2018-11-19T00:00:00Z","description":"2018-11-19","keywords":null,"link":"/blog/2018-11-19-dynamodbstreams-lambda-sns-sqs/","tags":["aws","dynamodb","dynamodbstreams","lambda","sns","sqs"],"text":"I uploaded an example code on how to monitor a Dynamodb table (INSERT, MODIFY, and REMOVE) using dynamodbstreams, lambda, SNS, and SQS. Please have a look here.","title":"Monitoring dynamodb table using dynamodbstream, lambda, SNS and SQS"},{"categories":null,"date":"2018-10-05T00:00:00Z","description":"2018-10-05","keywords":null,"link":"/blog/2018-10-05-aws-sqs-lengthy-process/","tags":["aws","sqs","client","subscription","lengthy"],"text":"Before, I uploaded an example on how to handle lengthy processing of PubSub subscription messages in GitHub. This time, it‚Äôs for AWS SQS. Have a look at the sample code here.","title":"AWS SQS lengthy message processing"},{"categories":null,"date":"2018-09-25T00:00:00Z","description":"2018-09-25","keywords":null,"link":"/blog/2018-09-25-ouchan-monorepo/","tags":["go","golang","monorepo","mobingi"],"text":"I posted an article for @mobingi about its Golang-based monorepo. Please check it out. Thanks.","title":"Mobingi‚Äôs golang monorepo"},{"categories":null,"date":"2018-09-20T00:00:00Z","description":"2018-09-20","keywords":null,"link":"/blog/2018-09-20-vimgo-go111-gocode/","tags":["vim","vim-go","gocode","autocomplete"],"text":"I use vim with vim-go as my main editor for Go-based projects (and for anything else, really). Recently, I‚Äôve upgraded to Go1.11 and my autocompletion stopped working. I know vim-go uses gocode as it‚Äôs autocompletion engine but since vim-go‚Äôs :GoInstallBinaries command handled the installation for me, I didn‚Äôt really have to think about it. Fortunately, a quick update to my vimrc file did the trick for me. I just had to add Plugin 'mdempsky/gocode', {'rtp': 'vim/'} to my vimrc (I use Vundle) and run :PluginInstall again. Works a treat.","title":"Autocompletion with gocode and vim-go in Go1.11"},{"categories":null,"date":"2018-08-13T00:00:00Z","description":"2018-08-13","keywords":null,"link":"/blog/2018-08-13-gcp-pubsub-lengthy-process/","tags":["gcp","pubsub","client","subscription","lengthy"],"text":"I have uploaded an example of how to handle lengthy processing of PubSub subscription messages in GitHub. If you are using PubSub and are, at times, struggling with PubSub default deadlines, you might find this example helpful. The code can be found here.","title":"GCP PubSub lengthy message processing"},{"categories":null,"date":"2018-07-24T00:00:00Z","description":"2018-07-24","keywords":null,"link":"/blog/2018-07-24-kubepfm/","tags":["kubepfm","kubectl","k8s","kubernetes","port-forward"],"text":"I recently uploaded a tool to GitHub that wraps kubectl port-forward command and supports port-forwarding to multiple pods. It‚Äôs called kubepfm. I use this tool heavily in my development work. You can check out the code here. You might find this useful.","title":"kubepfm, a kubectl port-forward wrapper for multiple pods"},{"categories":null,"date":"2018-06-09T00:00:00Z","description":"2018-06-09","keywords":null,"link":"/blog/2018-06-09-google-api-client-go-auth/","tags":["gcp","google","api","authentication","service-account"],"text":"This post is specifically for the autogenerated Google APIs Client for Go. I haven‚Äôt tried the other Google Cloud Library for Go since it didn‚Äôt have the compute library I needed. You can use the golang.org/x/oauth2/google library for authentication with this library. It can work with service accounts as well, which is what I am using at the moment. Using the GOOGLE_APPLICATION_CREDENTIALS environment variable After you have created your service account, downloaded the JSON file, and saved it in some location, you can set the GOOGLE_APPLICATION_CREDENTIALS environment variable with the path of your service account JSON file. # sample path is used $ export GOOGLE_APPLICATION_CREDENTIALS=/tmp/service-acct.json With that in place, the Application Default Credentials Example sample code should just work for you. Using the service account file directly to create the client It‚Äôs all fine and good if you only use one service account file all throughout the application but if you happen to need to use a specific JSON file per call, this section is for you.","title":"Authenticating Google API Client Library for Go using Service Accounts"},{"categories":null,"date":"2018-05-06T00:00:00Z","description":"2018-05-06","keywords":null,"link":"/blog/2018-05-06-xenial-to-bionic-lts-server-upgrade-zfs/","tags":["ubuntu","upgrade","xenial","bionic"],"text":"I just upgraded my home server from Ubuntu 16.04 LTS to 18.04 LTS without any problems so far. For context, this server runs a Samba file server using ZFS. Although after update, I had to run the following command to my pool: # pool name is 'm0' $ sudo zpool upgrade m0 By the way, at the time of this writing, Ubuntu Server update is not yet available (I think you have to wait for 18.04.1). I used the following command to update mine: $ do-release-upgrade -m server -d","title":"Ubuntu 16.04 LTS to Ubuntu 18.04 LTS"},{"categories":null,"date":"2018-04-30T00:00:00Z","description":"2018-04-30","keywords":null,"link":"/blog/2018-04-30-camera-class-filter-driver/","tags":["wdm","filter-driver","camera","driver","windows"],"text":"This post is a bit of a departure from my usual golang/cloud-related ramblings. I posted an open-source camera class filter driver for Windows ages ago hoping that it would help someone working on a similar project. If you are familiar with this type of driver, you probably know that it‚Äôs not that straightforward to write mainly due to it being generally undocumented. A lot of reverse engineering has been done to write this driver. Anyway, recently, someone pointed out to me that it‚Äôs been discussed in this forum post. So if you‚Äôre working on a similar project, give it a whirl, and if you find some issues, I would appreciate it if you could submit a PR for fixes. Thanks.","title":"Camera Class Filter Driver for Windows"},{"categories":null,"date":"2018-03-31T00:00:00Z","description":"2018-03-31","keywords":null,"link":"/blog/2018-03-31-access-pods-k8s/","tags":["kubectl","Kubernetes","ingress","port-forward","nginx"],"text":"At Mobingi, when we are developing services that run on Kubernetes, we generally use Minikube or Kubernetes in Docker for Mac. We also have a cluster that runs on GKE that we use for development. In this post, I will share how we access some of the services that are running on our development cluster. Using kubectl port-forward Using kubectl port-forward is probably the cheapest and the most straightforward. For example, if I want to access a cluster service svc1 through my localhost, I use kubectl port-forward like this: $ kubectl get pod NAME READY STATUS RESTARTS AGE svc1-66dd787767-d6b22 2/2 Running 0 7d svc1-66dd787767-ks92f 2/2 Running 0 7d svc2-578786c554-rlw2w 2/2 Running 0 7d # This will connect to the first pod (we have two available): $ kubectl port-forward `kubectl get pod --no-headers=true -o \\ custom-columns=:metadata.name | grep svc1 | head -n1` 8080:8080 Forwarding from 127.0.0.1:8080 -\u003e 8080 The left 8080 is my local port, the right 8080 is the pod port where svc1 is running. One thing to note with kubectl port-forward through is that it won‚Äôt disconnect automatically even when the pod is restarted, say for example, due to an update from CI. I have to restart the command by doing a Ctrl+C and then rerun. Exposing the service using NodePort or LoadBalancer This part is probably the easiest to setup. You can check the details from the Kubernetes documentation. But you have to be careful though, especially with load balancers. These are not cheap. We have gone with this route during our early Kubernetes days and we ended up with a lot of load balancers. This was when our clusters were still in AWS. In AWS, (I‚Äôm not sure if it is still the case now) when you specify LoadBalancer as service type, a classic load balancer will be provisioned for your service. That means one load balancer per exposed service! When we moved to GKE, we started using GLBC which uses an L7 load balancer via the Ingress API. This improved our costs a little bit since GLBC can support up to five backend services per load balancer using paths. The slight downside was that Ingress updates were a bit slow. It‚Äôs not a big deal though since it‚Äôs only in the development cluster and we use blue/green deployment in production. But still, some updates can take up to ten minutes. Using nginx as a reverse proxy to cluster services In our quest to further minimize costs, we are currently using nginx as our way of exposing services. We provisioned a single Ingress that points to an nginx service which serves as a reverse proxy to our cluster services. This was the cheapest for us as we only have one load balancer for all services. And updating the nginx reverse proxy service takes only a few seconds. So far, this worked for us with no significant problems for the past couple of months. Here‚Äôs an example of an nginx reverse proxy service: apiVersion: v1 kind: ConfigMap metadata: name: serviceproxy-conf data: serviceproxy.conf: | server { listen 80; server_name development.mobingi.com; resolver kube-dns.kube-system.svc.cluster.local valid=10s; location ~ ^/svc1/(.*)$ { proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $host; rewrite ^/svc1/(.*)$ /$1 break; proxy_pass \"http://svc1.default.svc.cluster.local\"; proxy_http_version 1.1; } location ~ ^/svc2/(.*)$ { proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $host; rewrite ^/svc2/(.*)$ /$1 break; proxy_pass \"http://svc2.default.svc.cluster.local\"; proxy_http_version 1.1; } # root health check requirement in GKE ingress location / { return 200 'healthy\\n'; } } --- apiVersion: apps/v1 kind: Deployment metadata: name: serviceproxy spec: replicas: 1 revisionHistoryLimit: 3 selector: matchLabels: app: serviceproxy template: metadata: labels: app: serviceproxy spec: containers: - name: nginx image: nginx:1.13 ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: /etc/nginx/conf.d/ volumes: - name: config-volume configMap: name: serviceproxy-conf items: - key: serviceproxy.conf path: serviceproxy.conf --- apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: serviceproxy-hpa namespace: default spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: serviceproxy minReplicas: 1 maxReplicas: 10 targetCPUUtilizationPercentage: 80 --- apiVersion: v1 kind: Service metadata: name: serviceproxy spec: type: NodePort ports: - name: http protocol: TCP port: 80 targetPort: 80 selector: app: serviceproxy In this example, all services, mainly svc1 and svc2, are running in the default namespace. Save this as service.yaml and deploy: $ kubectl create -f service.yaml A sample Ingress controller for the reverse proxy service: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: serviceproxy-ingress annotations: kubernetes.io/ingress.class: \"gce\" spec: tls: - secretName: mobingi-tls rules: - host: development.mobingi.com http: paths: - backend: serviceName: serviceproxy servicePort: 80 Save this as ingress.yaml and deploy: $ kubectl create -f ingress.yaml After everything is ready (Ingress provisioning takes some time), you should be able to access svc1 through https://development.mobingi.com/svc1/some-endpoint, svc2 through https://development.mobingi.com/svc2/another-endpoint, etc. Of course, you have to point your domain to your Ingress load balancer‚Äôs IP address which you can see using the following command: $ kubectl get ingress serviceproxy-ingress NAME HOSTS ADDRESS PORTS AGE serviceproxy-ingress development.mobingi.com 1.2.3.4 80, 443 91d If you‚Äôre wondering how to setup the TLS portion, you can refer to my previous post about the very subject.","title":"Accessing services in Kubernetes"},{"categories":null,"date":"2018-02-20T00:00:00Z","description":"2018-02-20","keywords":null,"link":"/blog/2018-02-20-k8s-tls-digicert/","tags":["DigiCert","Kubernetes","TLS"],"text":"It took me a while to make this work. I hope this will help someone out there who also is struggling with the same problem. We use DigiCert as our SSL certificate provider. The package I received contained three files: a keyfile, filename.key a certificate file, filename.crt an intermediate certificate file, DigiCertCA.crt I had to combine the two certificate files into a single file. I didn‚Äôt really check the order but I appended the intermediate certificate to my certificate file. Something like this: $ cp filename.crt tls.crt $ cat DigiCertCA.crt \u003e\u003e tls.crt $ cp filename.key tls.key $ kubectl create secret tls mytls --key tls.key --cert tls.crt I was able to successfully use the secret in a GCE Ingress: apiVersion: extensions/v1beta1 kind: Ingress ... spec: tls: - secretName: mytls backend: serviceName: myservice servicePort: 80 ...","title":"Creating a Kubernetes TLS secret using certificates from DigiCert"},{"categories":null,"date":"2018-02-06T00:00:00Z","description":"2018-02-06","keywords":null,"link":"/blog/2018-02-06-golang-monorepo/","tags":["go","golang","monorepo"],"text":"I uploaded an example of a golang-based monorepo in GitHub. This is a stripped down version of what we have @mobingi. If you are planning to have a golang-based monorepo, please have a look.","title":"A golang-based monorepo example"},{"categories":null,"date":"2018-01-24T00:00:00Z","description":"2018-01-24","keywords":null,"link":"/blog/2018-01-24-k8s-kops-aws/","tags":["kubernetes","kops","aws","devops"],"text":"Overview This post will walk through the steps on how we provisioned our production Kubernetes cluster @mobingi. Some of the bits here are already automated in our case but I will try to include as much details as I can. Our goals would be the following: Provision a Kubernetes cluster on AWS using kops. The cluster will have two autoscaling groups: one for on-demand, one for spot instances. It‚Äôs going to be a gossip-based cluster. RBAC is enabled in the cluster. There is no need to rewrite the otherwise excellent installation instructions written in the kops wiki. Basically, we can just follow the setup instructions there. The instructions in this post will be specific to our goals above. Gossip-based cluster We will skip the DNS configuration section as we are provisioning a gossip-based cluster, meaning, the cluster name will be something like \"\u003csome-name\u003e.k8s.local\". We will be using \"mycluster.k8s.local\" as our cluster name. SSH keypair We will create the keypair in AWS under EC2 -\u003e Key Pairs -\u003e Create Key Pair. We need this keypair when we need to ssh to our cluster nodes. After saving the keypair somewhere, we will generate the public key using the following command: $ ssh-keygen -y -f mycluster.pem \u003e mycluster.pem.pub Create the cluster At this point, we should already have our environment variables set, mainly NAME and KOPS_STATE_STORE. To create the cluster: # I'm from Japan so I'm using ap-northeast-1 (Tokyo) $ kops create cluster \\ --ssh-public-key mycluster.pem.pub --zones ap-northeast-1a,ap-northeast-1c \\ --authorization RBAC \\ --cloud aws ${NAME} \\ --yes It will take some time before the cluster will be ready. To validate the cluster: $ kops validate cluster Using cluster from kubectl context: mycluster.k8s.local Validating cluster mycluster.k8s.local INSTANCE GROUPS NAME ROLE MACHINETYPE MIN MAX SUBNETS master-ap-northeast-1a Master m3.medium 1 1 ... nodes Node t2.medium 2 2 ... NODE STATUS NAME ROLE READY ip-x-x-x-x.ap-northeast-1.compute.internal master True ip-x-x-x-x.ap-northeast-1.compute.internal node True ip-x-x-x-x.ap-northeast-1.compute.internal node True ... Your cluster mycluster.k8s.local is ready Notice that we only have one instance for our master. We can also opt to have a highly available master using these options, which is generally recommended for production clusters. Based on our experience though, this single master instance setup is good enough for development and/or staging clusters. There‚Äôs going to be downtime if master goes down in which case the duration will depend on how long AWS autoscaling group kicks in. During that window, k8s API won‚Äôt be accessible but the nodes will continue to work, including our deployed applications. Spot instance autoscaling group Once the cluster is ready, we will add another instance group for spot instances. The default instance group created in the previous command, named ‚Äúnodes‚Äù, will be our on-demand group. To add: $ kops create ig nodes-spots -subnet ap-northeast-1a,ap-northeast-1c You can then edit using the following contents (modify values as needed): apiVersion: kops/v1alpha2 kind: InstanceGroup metadata: creationTimestamp: \u003csome-datetime-value-here\u003e labels: kops.k8s.io/cluster: mycluster.k8s.local name: nodes-spot spec: image: kope.io/k8s-1.8-debian-jessie-amd64-hvm-ebs-2017-12-02 machineType: t2.medium maxPrice: \"0.02\" maxSize: 10 minSize: 2 nodeLabels: kops.k8s.io/instancegroup: nodes spot: \"true\" role: Node subnets: - ap-northeast-1a - ap-northeast-1c We can now update our cluster with the following commands: # [optional] update our on-demand group's max size to some number $ kops edit ig nodes # [optional] preview the changes to be applied $ kops update cluster ${NAME} # actual cluster update $ kops update cluster ${NAME} --yes # [optional] check if we need rolling update $ kops rolling-update cluster # if so, add --yes option $ kops rolling-update cluster --yes We can now validate the cluster to see our changes: $ kops validate cluster Validating cluster mycluster.k8s.local INSTANCE GROUPS NAME ROLE MACHINETYPE MIN MAX SUBNETS master-ap-northeast-1a Master m3.medium 1 1 ... nodes Node t2.medium 2 4 ... nodes-spot Node t2.medium 2 10 ... NODE STATUS NAME ROLE READY ip-x-x-x-x.ap-northeast-1.compute.internal master True ip-x-x-x-x.ap-northeast-1.compute.internal node True ip-x-x-x-x.ap-northeast-1.compute.internal node True ... Your cluster mycluster.k8s.local is ready When our cluster was created, kops also has automatically generated our config file for kubectl. To verify: $ kubectl cluster-info Kubernetes master is running at https... KubeDNS is running at https... To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Setup cluster autoscaler Clusters created using kops use autoscaling groups, but without scaling policies (at the time of writing). To enable dynamic scaling of our cluster, we will use cluster autoscaler. Before cluster autoscaler deployment, we need to setup some prerequisites. First, we need to attach the following permissions to master and nodes IAM role. Go to IAM roles console and add an inline policy to the roles created by kops. Role names would be something like: master.mycluster.k8s.local nodes.mycluster.k8s.local { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"autoscaling:DescribeAutoScalingGroups\", \"autoscaling:DescribeAutoScalingInstances\", \"autoscaling:DescribeTags\", \"autoscaling:SetDesiredCapacity\", \"autoscaling:TerminateInstanceInAutoScalingGroup\" ], \"Resource\": \"*\" } ] } The latest installation instructions can be found here. The general idea is to choose the latest yaml file, update it with your own values, and apply the file using kubectl. The readme file also provides a script that does the download and edit for us. We will be using the following command line arguments for our autoscaler: command: - ./cluster-autoscaler - --cloud-provider=aws - --v=4 - --stderrthreshold=info - --scale-down-delay=5m - --skip-nodes-with-local-storage=false - --expander=least-waste - --nodes=2:4:nodes.mycluster.k8s.local - --nodes=2:10:nodes-spot.mycluster.k8s.local You should update the last two line with your own autoscaling group min/max values. Finally, we deploy our autoscaler with: $ kubectl create -f cluster-autoscaler.yml deployment \"cluster-autoscaler\" created That‚Äôs it. You may also want to install these addons if you like. SSH to a node # sample ip's only $ kubectl get node -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE ... ip-x.x.x.x.ap-northeast-1... Ready master 1d v1.8.6 1.2.3.4 Debian GNU/Linux 8 (jessie) ... ip-x.x.x.x.ap-northeast-1... Ready node 1d v1.8.6 1.2.3.5 Debian GNU/Linux 8 (jessie) ... ip-x.x.x.x.ap-northeast-1... Ready node 1d v1.8.6 1.2.3.6 Debian GNU/Linux 8 (jessie) ... ip-x.x.x.x.ap-northeast-1... Ready node 1d v1.8.6 1.2.3.7 Debian GNU/Linux 8 (jessie) ... ip-x.x.x.x.ap-northeast-1... Ready node 1d v1.8.6 1.2.3.8 Debian GNU/Linux 8 (jessie) ... # default username is `admin` $ ssh -i mycluster.pem admin@1.2.3.4","title":"Running Kubernetes on AWS using kops"},{"categories":null,"date":"2017-12-01T00:00:00Z","description":"2017-12-01","keywords":null,"link":"/blog/2017-12-01-golang-cobra-glog/","tags":["golang","glog","cobra"],"text":"If you have been programming with golang, you‚Äôve probably heard of cobra. I use it extensively at work and also in my personal projects. Recently though, I‚Äôve been using glog more and more. And I quite like it. The thing is, it has a couple of flag definitions in its init() function using golang‚Äôs builtin flag library. And I wanted to include those flags into cobra‚Äôs flag definitions. This is how I did it. Generated help information will now look something like this. # run the help command $ ./cobraglog -h Use glog with cobra. Usage: [flags] Flags: --alsologtostderr log to standard error as well as files --echo string echo string (default \"hello\") --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory --logtostderr log to standard error instead of files --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging -h, --help help for this command Note that our cobra-defined flag --echo is also there. The rest are defined by glog internally. Finally, run the application. # run the binary, providing the logtostderr flag defined by glog $ ./cobraglog --logtostderr I1129 13:49:34.166660 2138 main.go:28] echo (info): hello W1129 13:49:34.166718 2138 main.go:29] echo (warn): hello E1129 13:49:34.166722 2138 main.go:30] echo (error): hello Here‚Äôs another example using subcommands.","title":"Using glog together with cobra in golang"},{"categories":null,"date":"2017-11-28T00:00:00Z","description":"2017-11-28","keywords":null,"link":"/blog/2017-11-28-embed-github-code/","tags":["gist-it","github-pages","jekyll"],"text":"I‚Äôve found this nifty little tool that can embed codes directly from your GitHub repositories without using Gist. It‚Äôs very easy to use. For example, if you want to embed, say, this whole file, you only need to add this snippet somewhere in your post.md file. \u003cscript charset=\"UTF-8\" src=\"https://gist-it.appspot.com/github.com/flowerinthenight/rusttrace/blob/master/src/main.rs?footer=minimal\"\u003e\u003c/script\u003e The ?footer=minimal part is optional. It will look something like this: It can also embed just a section of code based on line numbers. For example, to embed line 234 to line 257 of this file, this snippet will do. \u003cscript charset=\"UTF-8\" src=\"https://gist-it.appspot.com/github.com/flowerinthenight/rmq/blob/master/rabbitmq.go?slice=233:257\u0026footer=minimal\"\u003e\u003c/script\u003e It will look something like this: Just note that the slice= parameter is zero-based and GitHub‚Äôs line numbering starts at 1. Finally, if you have an HTTPS blog, make sure to use https://gist-it.appspot.com/..., not http://gist-it.appspot.com/....","title":"Embed code from GitHub to GitHub Pages blog like Gist"},{"categories":null,"date":"2017-10-30T00:00:00Z","description":"2017-10-30","keywords":null,"link":"/blog/2017-10-30-nativestore/","tags":["go","nativestore","keychain","wincred","pass"],"text":"This post is to show a simple way of using Docker‚Äôs credential helper package to utilize the system‚Äôs native credential store as storage for your Golang-based CLI applications‚Äô login credentials. This means Keychain for OSX, wincred for Windows, and pass for Linux. We use pass here since secretservice, although supported, doesn‚Äôt work out of the box in headless servers. Here‚Äôs a simple implementation of our Set, Get, and Del functions. Then we create our _darwin.go, _linux.go, and _windows.go files for OS specific implementations. Here‚Äôs a quickstart guide to setup pass in Ubuntu systems. # install pass $ sudo apt-get install pass # generate your own key using gpg2, do not use a passphrase $ gpg2 --gen-key # if the cmd seems stuck due to lack of entropy, you can open another window and run the ff cmd: # dd if=/dev/sda of=/dev/zero # list your keys $ gpg2 --list-keys /home/user/.gnupg/pubring.kbx ------------------------------ pub rsa2048/5486B0F6 2017-09-22 [SC] uid [ultimate] IamGroot \u003ciamgroot@domain.com\u003e sub rsa2048/CDC4C430 2017-09-22 [E] # initialize pass (use the pub key id) $ pass init 5486B0F6 Here‚Äôs an example on how to use our nativestore functions. Finally, you can refer to the whole package here.","title":"Using OS specific stores for storing CLI credentials for golang apps"},{"categories":null,"date":"2017-09-20T00:00:00Z","description":"2017-09-20","keywords":null,"link":"/blog/2017-09-20-docker-etwlogs/","tags":["docker","go","windows","etw"],"text":"In Docker‚Äôs ETW logging driver doc, it uses the tool logman to view the logs. In this article, I will show you how to use mftrace to view Docker ETW logs in real-time. First, here‚Äôs a simple application written in Go that logs to STDERR every second. Next, let‚Äôs create a Docker image (Windows) using the Dockerfile below. # assuming the code above is saved in a directory called 'demoapp' $ docker build -t demoapp . To use mftrace, we need a config file. Open a command prompt (or Powershell) and run the following command. $ mftrace.exe -c config.xml Then open another command prompt (or Powershell) window and run the Docker image. $ docker run -d --log-driver=etwlogs --name demoapp demoapp:latest You should be able to view the application logs in the mftrace window. You can use this repo instead of creating your own folder structure. Instructions are provided in the README as well as an x86 version of mftrace.","title":"Using Docker‚Äôs ETW log driver in Windows"},{"categories":null,"date":"2017-08-31T00:00:00Z","description":"2017-08-31","keywords":null,"link":"/blog/2017-08-31-golang-json-prettify/","tags":["go","json","prettify"],"text":"For self reference:","title":"JSON prettifier function in Go"},{"categories":null,"date":"2017-07-31T00:00:00Z","description":"2017-07-31","keywords":null,"link":"/blog/2017-07-31-ghpages-netlify/","tags":["website","netlify","https"],"text":"As of this writing, GitHub Pages still doesn‚Äôt support HTTPS for custom domains. Last week, I came across Netlify being able to serve GitHub Pages as is, with HTTPS support even with custom domains. I gave it a go. And it worked. If you‚Äôre in the same boat as me (using GH-Pages with custom domains), give Netlify a go. Note: I am not affiliated with Netlify in any way, whatsoever.","title":"From GitHub Pages to Netlify"},{"categories":null,"date":"2017-06-30T00:00:00Z","description":"2017-06-30","keywords":null,"link":"/blog/2017-06-30-amqp-wrapper/","tags":["go","amqp","rabbitmq"],"text":"I uploaded a simple wrapper to streadway/amqp library for RabbitMQ with support for auto reconnections. This is a simplified version of what I‚Äôm using in production.","title":"Wrapper for amqp library for RabbitMQ"},{"categories":null,"date":"2017-05-31T00:00:00Z","description":"2017-05-31","keywords":null,"link":"/blog/2017-05-31-start-process-as-system/","tags":["c++","windows","process"],"text":"Note that this function assumes the caller to be running as SYSTEM as well (i.e. Windows service). For self reference:","title":"Start process as system using CreateProcessAsUser"},{"categories":null,"date":"2017-04-20T00:00:00Z","description":"2017-04-20","keywords":null,"link":"/blog/2017-04-20-apc-as-fifo-queue/","tags":["c++","fifo","queue"],"text":"In this way, we can implement a FIFO queue without using explicit locking/synchronization for enqueueing/dequeueing. For self reference:","title":"Using APC as FIFO queue in Windows"},{"categories":null,"date":"2017-03-28T00:00:00Z","description":"2017-03-28","keywords":null,"link":"/blog/2017-03-28-simple-touch-input-funcs-windows/","tags":["c++","windows","input"],"text":"For self reference: Simulate a touch input down, drag, up programmatically.","title":"Simple touch/swipe input function in Windows"},{"categories":null,"date":"2017-02-28T00:00:00Z","description":"2017-02-28","keywords":null,"link":"/blog/2017-02-28-simple-innosetup/","tags":["innosetup","installer","pascal"],"text":"InstallShield seems ubiquitous when it comes to installer scripting, at least based on my experience with companies I‚Äôve worked for so far. But for my side projects, I‚Äôve always been a fan of InnoSetup. It is simple to use and Pascal is not really that bad. It is free and can do probably all the things you require for an installer. Here‚Äôs an example of an InnoSetup script that I have been using as baseline for creating installers for Windows.","title":"Simple Innosetup installation script"},{"categories":null,"date":"2017-01-18T00:00:00Z","description":"2017-01-18","keywords":null,"link":"/blog/2017-01-18-base-atl-service/","tags":["c++","windows","atl","service"],"text":"Yet another service code for Windows. This time, it‚Äôs an ATL service. ATL services are basically the same as traditional Windows services but with some advantages. Can be started on-demand automatically by the first client call (via COM). Clients can call service functions with parameters and return values using COM. In traditional services, clients normally communicate using service control codes and you need some kind of IPC (named pipes, shared memory, etc.) for bi-directional data exchange. A client console app is provided to demonstrate service function call with return value and service-to-client notification via IDispatch. Check out the source code here.","title":"ATL service base code"},{"categories":["books","reading"],"date":"2017-01-04T00:00:00Z","description":"2017-01-04","keywords":null,"link":"/blog/2017-01-04-bookshelf/","tags":null,"text":"Happy new year to all! To the bookworms out there, I‚Äôve added a new page in the site called Bookshelf, a list of all the books I‚Äôve read so far. If you have any suggestions/recommendations, please don‚Äôt hesitate to leave a comment. Thank you.","title":"New ‚ÄòBookshelf‚Äô page"},{"categories":["rust","c++","etw"],"date":"2016-12-26T00:00:00Z","description":"2016-12-26","keywords":null,"link":"/blog/2016-12-26-rust-etw/","tags":null,"text":"I‚Äôve been dabbling a bit with Rust and so far, I‚Äôm liking it. I‚Äôll probably use it more as replacement for my C/C++ projects. And I just uploaded a very simple ETW wrapper I‚Äôm using in one of my side projects. You can check out the source code here.","title":"Simple ETW wrapper for Rust"},{"categories":["Code","C#","C++"],"date":"2016-11-28T00:00:00Z","description":"2016-11-28","keywords":null,"link":"/blog/2016-11-28-ffmpeg-encode-h264mp4/","tags":null,"text":"Recently, I was working on a project that involves the encoding of .NET bitmaps using ffmpeg‚Äôs h264 encoder with mp4 as container. This video output will be used in a \u003cvideo\u003e tag in html5. Sample codes have been all over the place so it took me quite a while to come up with a working solution. The official sample from ffmpeg only encodes to raw h264 stream. This is just a basic sample code for reference. Check out the source code here. Lastly, some useful links that I used: https://en.code-bude.net/2013/04/17/how-to-create-video-files-in-c-from-single-images/ https://github.com/FFmpeg/FFmpeg/blob/master/doc/examples/decoding_encoding.c http://www.imc-store.com.au/Articles.asp?ID=276","title":"Encoding .NET bitmaps to H264 using FFMPEG"},{"categories":["Code","Go"],"date":"2016-10-07T00:00:00Z","description":"2016-10-07","keywords":null,"link":"/blog/2016-10-07-cron-like-windows-service-go/","tags":null,"text":"A couple of days ago, I uploaded a generic service in GitHub that has a feature of updating itself. Check out the post here. Today, I uploaded the actual service I‚Äôm using in our CI environment, excluding the configuration file. Check out the source code here.","title":"A simple cron-like Windows service"},{"categories":["Code","Go"],"date":"2016-09-27T00:00:00Z","description":"2016-09-27","keywords":null,"link":"/blog/2016-09-27-windows-delay-update-service-go/","tags":null,"text":"Recently, I‚Äôve been working on a service that runs on a lot of VM‚Äôs across different locations. If I have a new service build, updating all of the running instances quickly became a bit of a pain. I have to log in to every VM (in some cases through a VPN) and then do a manual upgrade. Now, there are probably tools that already exist for this type of use case but since I‚Äôm still learning Go at the moment, I thought this would be a good exercise. Basically, this is a Windows service that has an http endpoint that accepts a file upload (in this case, a new version of itself). The service then saves this file, calls the MoveFileEx API with the MOVEFILE_DELAY_UNTIL_REBOOT flag, then reboots the system. I also added a simple client (still written in Go) that will upload the file. With these tools, I can now update all the running service instances with a script. Full source code here.","title":"A Windows service with an http endpoint for uploading a new version of itself"},{"categories":["Code","C#"],"date":"2016-08-24T00:00:00Z","description":"2016-08-24","keywords":null,"link":"/blog/2016-08-24-rest-client-server-c%23/","tags":null,"text":"For personal reference: Full source code here.","title":"A simple REST client/server console app in C#"},{"categories":["Code","golang"],"date":"2016-08-22T00:00:00Z","description":"2016-08-22","keywords":null,"link":"/blog/2016-08-22-syslog-target-go/","tags":null,"text":"For personal reference: To view syslog in realtime tail -f /var/log/syslog","title":"Syslog as target in Go logs"},{"categories":["Code","golang"],"date":"2016-08-21T00:00:00Z","description":"2016-08-21","keywords":null,"link":"/blog/2016-08-21-log-prefix-fname-go/","tags":null,"text":"For personal reference:","title":"Add function name prefix to log.Println in Go"},{"categories":["Code","golang"],"date":"2016-08-20T00:00:00Z","description":"2016-08-20","keywords":null,"link":"/blog/2016-08-20-expvar-gorilla-mux/","tags":null,"text":"For personal reference: Access root http://localhost:8000 Access expvar information http://localhost:8000/debug/vars","title":"How to serve expvar when using gorilla/mux"},{"categories":["Code","golang"],"date":"2016-08-19T00:00:00Z","description":"2016-08-19","keywords":null,"link":"/blog/2016-08-19-dir-cleanup-tool/","tags":null,"text":"For personal reference:","title":"Simple directory cleanup tool for Windows (golang)"},{"categories":["Code"],"date":"2016-08-18T00:00:00Z","description":"2016-08-18","keywords":null,"link":"/blog/2016-08-18-git-commands-notes/","tags":null,"text":"Update 2017/03/24: Transferred to a separate repository here. For personal reference: Reset a file git checkout HEAD -- my-file.txt Delete last commit git reset --hard HEAD~1 Delete local branch git branch -d \u003cbranch-name\u003e or to force delete git branch -D \u003cbranch-name\u003e Delete branch from remote repository git push origin --delete \u003cremote-branch-name\u003e Search for the merge commit from a specific commit git log \u003cSHA\u003e..master --ancestry-path --merges Search for a commit message git log | grep \u003cpattern\u003e List commits on range line of codes for one file git blame -L\u003cline#\u003e,+\u003coffset\u003e -- \u003cfilename\u003e For example, three lines starting from line 257 of main.cpp git blame -L257,+3 -- main.cpp History of a line (or lines) in a file git log --topo-order --graph -u -L \u003cline-start\u003e,\u003cline-end\u003e:\u003cfile\u003e For example, history of line 155 of main.cpp git log --topo-order --graph -u -L 155,155:main.cpp Compare (diff) a file from the current branch to another branch git diff ..\u003ctarget-branch\u003e \u003cpath-to-file\u003e Or if difftool is configured git difftool ..\u003ctarget-branch\u003e \u003cpath-to-file\u003e Rebase/squash all branch commits git checkout -b new-branch modify... commit... ... git rebase -i master (sometimes, I branch out of master for a clean branch and do a git rebase -i clean-branch) git checkout master git rebase new-branch (delete clean-branch) Combine all branch commits to one before merging to master (sort of like the one above) git checkout master git checkout -b clean git merge --squash branch_to_merge_to_one_commit git commit (add commit message) git checkout master git merge clean Custom format for log Add to global .gitconfig using git config --global alias.logp \"...\" git log --pretty=format:'%Cred%h %C(yellow)%d%Creset %s %Cgreen(%cr|%ci) %C(bold blue)[%an]%Creset'","title":"My commonly used commands in GIT"},{"categories":["Code"],"date":"2016-08-13T00:00:00Z","description":"2016-08-13","keywords":null,"link":"/blog/2016-08-13-signal-handler-linux-cpp/","tags":null,"text":"Personal reference:","title":"A simple signal handler in C/C++ (Linux)"},{"categories":["Code"],"date":"2016-08-11T00:00:00Z","description":"2016-08-11","keywords":null,"link":"/blog/2016-08-11-sharing-my-tmux-conf/","tags":null,"text":"I use tmux heavily and in tandem with vim. Much more so now when it‚Äôs supported on Bash on Windows as well. I don‚Äôt have to spin up a Linux VM just for the purpose of being my tmux ‚Äúserver‚Äù. # Set a Ctrl-b shortcut for reloading tmux config unbind r bind r source-file ~/.tmux.conf # Prefix is Ctrl-a unbind C-b set -g prefix C-a bind C-a send-prefix # Rename terminals set -g set-titles on set -g set-titles-string '#(whoami)@#h@#(curl ipecho.net/plain;echo)' # Status bar customization set -g status-bg black set -g status-fg white set -g status-interval 5 set -g status-left-length 90 set -g status-right-length 60 set -g status-left \"#[fg=Green]#(whoami)#[fg=white]@#[fg=red]#(hostname -s)#[fg=white]|#[fg=yellow]#(curl ipecho.net/plain;echo)#[fg=white]|#[fg=yellow]#(hostname -I)#[fg=white]\" set -g status-justify left set -g status-right '#[fg=Cyan]#S #[fg=white]%a %d %b %R' # Easy to remember split pane commands bind | split-window -h bind - split-window -v unbind '\"' unbind % # Vim friendly settings (from https://gist.github.com/anonymous/6bebae3eb9f7b972e6f0) setw -g monitor-activity on set -g visual-activity on set -g mode-keys vi # Extend history limit set -g history-limit 10000","title":"Sharing my .tmux.conf"},{"categories":["Code"],"date":"2016-07-27T00:00:00Z","description":"2016-07-27","keywords":null,"link":"/blog/2016-07-27-sharing-my-vimrc/","tags":null,"text":"Vim has always been my go to editor/IDE when I‚Äôm outside of Visual Studio. Here‚Äôs my base _vimrc for Windows. let mapleader = \" \" filetype off syntax on colorscheme darkblue \" let pc=$PC \" if pc == 'HOME' \" set guifont=Letter\\ Gothic\\ Std:h11 \" else \" set guifont=Lucida\\ Sans\\ Typewriter:h9 \" endif \" Save marks to up to 100 files, save global marks as well (f1). To disable, f0 set viminfo='100,f1 \" Folding options set foldmethod=indent set foldnestmax=20 set nofoldenable set foldlevel=0 set guifont=Lucida\\ Sans\\ Typewriter set lines=70 columns=160 set ai set nu set tabstop=4 set shiftwidth=4 set softtabstop=4 set expandtab set wrap set backspace=2 set encoding=utf-8 set fileencodings=utf-8 set nocompatible set noswapfile set shortmess+=I set ignorecase set guioptions-=T set guioptions-=r set guioptions-=m set splitright set splitbelow set ruler set rtp+=$HOME/vimfiles/bundle/Vundle.vim/ call vundle#begin('$USERPROFILE/vimfiles/bundle/') Plugin 'VundleVim/Vundle.vim' \" 1. Vim should be 64-bit (link in ycm site) \" 2. Python should be 64-bit Plugin 'Valloric/YouCompleteMe' Plugin 'fatih/vim-go' Plugin 'PProvost/vim-ps1' call vundle#end() filetype plugin indent on \" To ignore plugin indent changes, instead use: \" filetype plugin on \" let g:netrw_liststyle=3 let g:ycm_disable_for_files_larger_than_kb = 0 let g:ycm_autoclose_preview_window_after_completion = 1 \" Enable powershell syntax plug autocmd BufNewFile,BufReadPost *.ps1 set filetype=ps1 \" Search for the word under cursor in the current dir (recursively) command CSM :execute \"vimgrep /\" . expand(\"\u003ccword\u003e\") . \"/j ** \u003cBar\u003e :cw\" nnoremap \u003cleader\u003ems :CSM\u003cCR\u003e \" Simple mappings for window manipulations nnoremap \u003cleader\u003ewq \u003cC-W\u003eq nnoremap \u003cleader\u003ews \u003cC-W\u003es nnoremap \u003cleader\u003ewv \u003cC-W\u003ev nnoremap \u003cleader\u003e\u003cleft\u003e\u003cleft\u003e \u003cC-W\u003e\u003cleft\u003e nnoremap \u003cleader\u003e\u003cright\u003e\u003cright\u003e \u003cC-W\u003e\u003cright\u003e nnoremap \u003cleader\u003e\u003cup\u003e\u003cup\u003e \u003cC-W\u003e\u003cup\u003e nnoremap \u003cleader\u003e\u003cdown\u003e\u003cdown\u003e \u003cC-W\u003e\u003cdown\u003e \" Display buffers, then prep the colon for the next command nnoremap \u003cleader\u003eb :ls\u003cCR\u003e: \" Shortcut for save nnoremap \u003cleader\u003es :w\u003cCR\u003e \" Shortcut for netrw explorer nnoremap \u003cleader\u003ee :e.\u003cCR\u003e \" JSON pretty print (all buffer) nnoremap \u003cleader\u003epj :%!python -m json.tool\u003cCR\u003e \" Diff all windows (should prep 2 windows for this) nnoremap \u003cleader\u003edt :windo diffthis\u003cCR\u003e nnoremap \u003cleader\u003edo :windo diffoff!\u003cCR\u003e {% endhighlight %} And here's my base `.vimrc` for Linux and OSX. {% highlight conf %} let mapleader = \" \" filetype off syntax on colorscheme elflord set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() Plugin 'VundleVim/Vundle.vim' Plugin 'Valloric/YouCompleteMe' Plugin 'jelera/vim-javascript-syntax' Plugin 'fatih/vim-go' Plugin 'majutsushi/tagbar' call vundle#end() filetype plugin indent on \" Save marks to up to 100 files, save global marks as well (f1). To disable, f0 set viminfo='100,f1 \" Folding options set foldmethod=indent set foldnestmax=20 set nofoldenable set foldlevel=0 set tabstop=4 set shiftwidth=4 set softtabstop=4 set expandtab set autoindent set nu set encoding=utf-8 set noswapfile set shortmess+=I set backspace=2 set nocompatible set ignorecase set splitright set splitbelow set ruler \" Temporary enable/disable YouCompleteMe. Active = disable, commented = enable \" let g:loaded_youcompleteme = 1 let g:ycm_global_ycm_extra_conf = \"~/.ycm_extra_conf.py\" let g:ycm_disable_for_files_larger_than_kb = 0 let g:ycm_autoclose_preview_window_after_completion = 1 \" Options for netrw let g:netrw_liststyle = 0 let g:netrw_altv = 1 \" 1. :PluginInstall \" 2. :GoInstallBinaries (gotags, vim-go) \" 3. Install 'ctags' \" 4. go get -u github.com/jstemmer/gotags let g:tagbar_type_go = { \\ 'ctagstype' : 'go', \\ 'kinds' : [ \\ 'p:package', \\ 'i:imports:1', \\ 'c:constants', \\ 'v:variables', \\ 't:types', \\ 'n:interfaces', \\ 'w:fields', \\ 'e:embedded', \\ 'm:methods', \\ 'r:constructor', \\ 'f:functions' \\ ], \\ 'sro' : '.', \\ 'kind2scope' : { \\ 't' : 'ctype', \\ 'n' : 'ntype' \\ }, \\ 'scope2kind' : { \\ 'ctype' : 't', \\ 'ntype' : 'n' \\ }, \\ 'ctagsbin' : 'gotags', \\ 'ctagsargs' : '-sort -silent' \\ } nnoremap \u003cF8\u003e :TagbarToggle\u003cCR\u003e \" Search for the word under cursor in the current dir (recursively) command CSM execute \":vimgrep /\" . expand(\"\u003ccword\u003e\") . \"/j ** \u003cBar\u003e :cw\" nnoremap \u003cleader\u003ems :CSM\u003cCR\u003e set wildignore+=jennah \" Simple mappings for window manipulations nnoremap \u003cleader\u003ewq \u003cC-W\u003eq nnoremap \u003cleader\u003ews \u003cC-W\u003es nnoremap \u003cleader\u003ewv \u003cC-W\u003ev nnoremap \u003cleader\u003e\u003cleft\u003e\u003cleft\u003e \u003cC-W\u003e\u003cleft\u003e nnoremap \u003cleader\u003e\u003cright\u003e\u003cright\u003e \u003cC-W\u003e\u003cright\u003e nnoremap \u003cleader\u003e\u003cup\u003e\u003cup\u003e \u003cC-W\u003e\u003cup\u003e nnoremap \u003cleader\u003e\u003cdown\u003e\u003cdown\u003e \u003cC-W\u003e\u003cdown\u003e \" Display buffers, then prep the colon for the next command nnoremap \u003cleader\u003eb :ls\u003cCR\u003e: \" Shortcut for save nnoremap \u003cleader\u003es :w\u003cCR\u003e \" Shortcut for netrw explorer nnoremap \u003cleader\u003ee :e.\u003cCR\u003e \" JSON pretty print (all buffer) nnoremap \u003cleader\u003epj :%!python -m json.tool\u003cCR\u003e \" Diff all windows (should prep 2 windows for this) nnoremap \u003cleader\u003edt :windo diffthis\u003cCR\u003e nnoremap \u003cleader\u003edo :windo diffoff!\u003cCR\u003e \" Highlight/no highlight for search nnoremap \u003cleader\u003ehl :set hlsearch\u003cCR\u003e nnoremap \u003cleader\u003ehn :set nohlsearch\u003cCR\u003e \" Diff all windows (should prep two windows for this) nnoremap \u003cleader\u003edt :windo diffthis\u003cCR\u003e nnoremap \u003cleader\u003edo :windo diffoff!\u003cCR\u003e Update (2016/08/24) Updated vimrc can be viewed here.","title":"Sharing my .vimrc"},{"categories":["Code"],"date":"2016-06-13T00:00:00Z","description":"2016-06-13","keywords":null,"link":"/blog/2016-06-13-folder-file-watcher-powershell/","tags":null,"text":"Personal reference:","title":"A simple folder/file watcher in Powershell"},{"categories":["Code"],"date":"2016-05-06T00:00:00Z","description":"2016-05-06","keywords":null,"link":"/blog/2016-05-06-import-environment-powershell/","tags":null,"text":"A couple of days ago, I was working on a powershell-based script for mstest automation and I needed to call vsdevcmd.bat from Visual Studio‚Äôs tools folder. function Invoke-Environment([Parameter(Mandatory=1)][string]$Command, [switch]$Output, [switch]$Force) { $stream = if ($Output) { ($temp = [IO.Path]::GetTempFileName()) } else { 'nul' } $operator = if ($Force) {'\u0026'} else {'\u0026\u0026'} foreach($_ in cmd /c \" $Command \u003e `\"$stream`\" 2\u003e\u00261 $operator SET\") { if ($_ -match '^([^=]+)=(.*)') { [System.Environment]::SetEnvironmentVariable($matches[1], $matches[2]) } } if ($Output) { Get-Content -LiteralPath $temp Remove-Item -LiteralPath $temp } } To use the function: Invoke-Environment '\"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools\\VsDevCmd.bat\"'","title":"A simple Powershell function to import an environment"},{"categories":["Japan"],"date":"2016-04-10T00:00:00Z","description":"2016-04-10","keywords":null,"link":"/blog/2016-04-10-trying-bash-on-windows/","tags":null,"text":"Downloading‚Ä¶ {% include image.html url=\"../../../../images/win10preview1.png\" %} And here we go‚Ä¶ {% include image.html url=\"../../../../images/win10preview2.png\" %} Looking good so far.","title":"Trying out Bash on Windows"},{"categories":["Code"],"date":"2016-04-06T00:00:00Z","description":"2016-04-06","keywords":null,"link":"/blog/2016-04-06-unzip-function-powershell/","tags":null,"text":"function UnzipFiles($ZipFileName, $DestDir) { Add-Type -Assembly System.IO.Compression.FileSystem [System.IO.Compression.ZipFile]::ExtractToDirectory($ZipFileName, $DestDir) } To use the function: UnzipFiles -ZipFileName .\\folder\\file.zip -DestDir .\\destination","title":"A simple unzip function in Powershell"},{"categories":["Code"],"date":"2016-04-05T00:00:00Z","description":"2016-04-05","keywords":null,"link":"/blog/2016-04-05-zip-function-powershell/","tags":null,"text":"function ZipFiles($ZipFileName, $SourceDir) { Add-Type -Assembly System.IO.Compression.FileSystem $compressionLevel = [System.IO.Compression.CompressionLevel]::Optimal [System.IO.Compression.ZipFile]::CreateFromDirectory($SourceDir, $ZipFileName, $compressionLevel, $false) } To use the function: ZipFiles -ZipFileName test.zip -SourceDir .\\folder\\to\\zip","title":"A simple zip function in Powershell"},{"categories":["Tech"],"date":"2016-03-28T00:00:00Z","description":"2016-03-28","keywords":null,"link":"/blog/2016-03-28-setup-samba-server/","tags":null,"text":"I installed Samba to my Ubuntu Server (which is an old ThinkPad laptop that has been gathering dust in my closet for ages): sudo apt-get install samba I have three 3-TB HDDs that I planned to use as my main server storage; one main server, and the other two for backup. All of these HDDs are in ext4 format. Then I created a new directory under /media where I will mount my main server storage: mkdir /media/myserver Then I edited the samba config file /etc/samba/smb.conf. I just scrolled to the very bottom and added these lines (after [print$] section): [MYSERVER] path = /media/myserver browsable = yes writable = yes guest ok = yes read only = no ``` Then I tested the config file for syntax errors using the command: ```sh testparm Before I added the mount entry for my server to /etc/fstab, I took note of my HDD‚Äôs UUID from the following command: sudo blkid in which the output looked something like /dev/sdb1: UUID=\"996a1b79-bf22-49fd-a3d1-33eab2708cfb\" TYPE=\"ext4\" Then I edited /etc/fstab and added the line below: UUID=996a1b79-bf22-49fd-a3d1-33eab2708cfb /media/myserver ext4 defaults 0 0 Lastly, I then rebooted my server. To access my server, I opened File Explorer and browsed to \\\\my_server_ip\\myserver.","title":"How I set up my Samba file server"},{"categories":["Code"],"date":"2016-03-22T00:00:00Z","description":"2016-03-22","keywords":null,"link":"/blog/2016-03-22-etw-part6/","tags":null,"text":"Check out the codes in GitHub. Basically, 2 steps: Create a C++ Windows Runtime Component (dll) that will use the ETW header file. Reference the WinRT DLL to C# project. WinRT DLL My logging class looks like this: public ref class TraceCore sealed { private: TraceCore(); ~TraceCore(); static TraceCore^ m_Instance; public: static property TraceCore^ Instance { TraceCore^ get() { if (m_Instance == nullptr) { m_Instance = ref new TraceCore(); } return m_Instance; } } void Verbose(Platform::String^ mod, Platform::String^ file, Platform::String^ func, Platform::String^ m); }; And heres the implementation: #include \"pch.h\" #include \"TraceCore.h\" using namespace LibRTWrapperETW; using namespace Platform; #include \"jytrace.h\" TraceCore^ TraceCore::m_Instance = nullptr; TraceCore::TraceCore() { EventRegisterJyTrace(); } TraceCore::~TraceCore() { EventUnregisterJyTrace(); } void TraceCore::Verbose(String^ mod, String^ file, String^ func, String^ m) { EventWriteSimple(mod-\u003eData(), file-\u003eData(), func-\u003eData(), L\"Trace\", m-\u003eData()); } Wrapper Class Just like in part 5, I wrapped the C++ bits to a C# class so I can use the CallerMemberName and CallerFilePath attributes: public static class TraceCoreWrapper { public static void VerboseCore( string m, [CallerMemberName] string memberName = \"?\", [CallerFilePath] string srcFile = \"?\", [CallerLineNumber] int srcNum = 0) { TraceCore.Instance.Verbose(\"CORE_RT [\" + Environment.CurrentManagedThreadId + \"]\", Path.GetFileName(srcFile), memberName, m); } } And finally, the actual logging in C#: TraceCoreWrapper.VerboseCore(\"Hello from UApp (CS) world!\");","title":"[Part 6] Logging with Universal Apps"},{"categories":["Code"],"date":"2016-03-19T00:00:00Z","description":"2016-03-19","keywords":null,"link":"/blog/2016-03-19-etw-part5/","tags":null,"text":"Check out the codes in GitHub. I generated a C# logging class using this command: mc -css \u003cNamespace_here\u003e jytrace.man So far, most of my event templates use an ANSI string data type which (I believe) is not supported by C#. As you can see in part 1, I use ANSI data type for my File and Function fields so that I can use __FILE__ and __FUNCTION__ as inputs in C++. Thats why I added a new event template with all fields using UNICODE data types. Again, you can refer to the whole package here for reference. To use the logging class, I added the generated file to my project. I also added a wrapper class so I can use the CallerMemberName and CallerFilePath attributes in C#. public static bool Verbose( string m, [CallerMemberName] string memberName = \"?\", [CallerFilePath] string srcFile = \"?\", [CallerLineNumber] int srcNum = 0) { return JyTrace.ProviderJyTrace.EventWriteSimple(\"ETWTest\", Path.GetFileName(srcFile), memberName, \"Trace:\", m); } The actual log call: TraceCore.Verbose(\"Hello from CS!\"); Check out part 6.","title":"[Part 5] Logging with C# applications"},{"categories":["Code"],"date":"2016-03-07T00:00:00Z","description":"2016-03-07","keywords":null,"link":"/blog/2016-03-07-etw-part4/","tags":null,"text":"Check out the codes in GitHub. Getting the logs Now that I have my modules spitting out logs for me, it‚Äôs time to actually consume (or view) them. Most of the time, I use MFTrace and logman tools. MFTrace Although MFTrace is primarily a tool for generating logs for Media Foundation apps, it is a great tool for viewing ETW logs in general as well. It is included in the MS SDK. To view logs in real time, I use this command in either Powershell or command line: mftrace -c config.xml To stop the trace collection, press CTRL+C. logman logman is a very powerful builtin performance counter and event trace log tool from Microsoft. For more information, have a look at here. You can use logman as alternative to MFTrace. Start tracing logman start \u003cname\u003e -p \u003cprovider_guid_or_name\u003e \u003ckw\u003e \u003clevel\u003e -o \u003coutput.etl\u003e -ets Examples logman start lms -p {3A8FD7D2-CAB3-455D-A8E5-9E1741365FEB} 0x1 win:Verbose -o c:\\output.etl -ets logman start lms -p MyProviderName 0x3 win:Informational -o c:\\output.etl -ets logman start lms -p {277c604b-1962-47fa-9307-7ce0855dfea6} 0xffffffffffffffff 0xff -o c:\\output.etl -ets Stop tracing logman stop \u003cname\u003e -ets Examples logman stop lms -ets Collecting ETW traces from test/production systems To collect ETW trace logs from test/production systems, manifest file and message/resource file need not be registered. mftrace -c config.xml -o c:\\output.etl c:\\output.etl is just an example. You can use any location and any filename as long as the extension is .etl. The output .etl file can only be read on a system where the manifest file and the resource/message file are registered. To read the traces: tracerpt -y output.etl The default readable output file that contains all the trace information will be dumpfile.xml. A summary.txt file will also be generated. For more information about tracerpt, have a look at here. Check out part 5.","title":"[Part 4] Log collection"},{"categories":["Code"],"date":"2016-03-03T00:00:00Z","description":"2016-03-03","keywords":null,"link":"/blog/2016-03-03-etw-part3/","tags":null,"text":"Check out the codes in GitHub. If you remember in part 2, we compiled our manifest file with mc -um \u003cmanifest_file\u003e.man and we got a header file as one of the outputs. We just have to include that header file to our sources and we are good to go. #include ... #include \"\u003cmanifest_file\u003e.h\" int main(...) { EventRegister\u003cprovider_name_in_manifest\u003e(); ... CreateFile(...); // Example of using the LastError event in our manifest file EventWriteLastError(L\"THIS_EXE\", __FILE__, __FUNC__, L\"CreateFile\", GetLastError()); ... EventUnregister\u003cprovider_name_in_manifest\u003e(); } Notes Any module can use the ETW provider, be it dll or exe, simultaneously. But you can also create a provider for each module if you prefer. For a dll, you can call the EventRegister() inside your DllMain -\u003e DLL_PROCESS_ATTACH and your EventUnregister() in DLL_PROCESS_DETACH. Check out part 4.","title":"[Part 3] Logging with C/C++ applications"},{"categories":["Code"],"date":"2016-03-02T00:00:00Z","description":"2016-03-02","keywords":null,"link":"/blog/2016-03-02-etw-part2/","tags":null,"text":"Check out the codes in GitHub. Compiling the manifest file This is how I compiled my manifest file. Open Visual Studio command prompt. Compile manifest file. mc -um \u003cmanifest_file\u003e.man When successful, output files are: \u003cfilename\u003e.h \u003cfilename\u003e.rc \u003cfilename\u003eTEMP.BIN MSG00001.bin Creating the message resource DLL I like to create a separate dll just for the manifest file resource although you can also add the .rc file to any of your existing dll or exe. Compile .rc to .res rc \u003cfilename\u003e.rc Create .dll from .res link -dll -noentry -out:\u003cout\u003e.dll \u003cfilename\u003e.res Register the manifest to the system (optional) I always register the manifest file on my development systems. As for client systems, its up to you. I dont. Open admin command prompt and execute wevtutil im \u003cmanifest_file\u003e.man /rf:\"full_path_to_resource_dll\" /mf:\"full_path_to_resource_dll\" Remove the manifest from the system Open admin command prompt and execute wevtutil um \u003cmanifest_file\u003e.man Check out part 3.","title":"[Part 2] Manifest file compilation and setup"},{"categories":["Code"],"date":"2016-03-01T00:00:00Z","description":"2016-03-01","keywords":null,"link":"/blog/2016-03-01-etw-part1/","tags":null,"text":"Check out the codes in GitHub. Ive been using ETW as my go-to logging mechanism for Windows development. I will be sharing how I setup my environment here. Creating the manifest file (.man) I use ecmangen.exe to create my manifest file. This tool is included in the SDK. I have Visual Studio 2015 installed on a Win10 64-bit machine, and its location is C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64\\. It looks something like this (my completed manifest): The red strip is the manifest file name, or my ETW provider name. I started by right-clicking Events Section -\u003e New -\u003e Provider, then gave it a name. Then prepended the string Provider to the existing name for the Symbol, auto-generated the GUID, left the other fields as blank, then Save. Setting up Keywords Keywords basically is a mask for filtering log outputs. Mine is just a simple logging mechanism so I have only two keywords. Im not even using KeywordFunctionEntryExit (set to 0x1) that much nowadays so KeywordGeneric (set to 0x2) for me is enough. With this, I have the option later to output only the logs with the keyword KeywordFunctionEntryExit or KeywordGeneric, or both. Setting up Templates Templates are, well, templates for the events (in this context, a log is basically an event). Templates will contain the fields (with data types) I want to include in my log. My goal was to have logs with a format of [module_name: src_file_name: function_name] key: value, where key would be any label, say, variable name, and value as, well, any value, be it int or bool, or last error, etc. I thought that this format is generic enough for me to log any information from my code. All events require a template (whether your template or default, which I admit I have never used). Multiple events can use a single template. Setting up Events Lastly, the events themselves. From the image above, I defined quite a number of events. If you noticed, my LastError event (which I use to log the GetLastError() API) specified the template TemplateKeyValueLastError and the keyword KeywordGeneric. When the manifest file is compiled, all the defined events by default (can be modified during compilation) will generate a logging API with the name EventWrite\u003cevent_name\u003e, which in the case of the highlighted one, EventWriteLastError(). Another thing to point out is the Message. The %[number] corresponds to the fields defined in the template used. First field will be %1, second field will be %2, and so on and so on. Lastly, save your manifest file. The xml file will have a .man extension. Check out part 2.","title":"[Part 1] How I set up ETW for logging (native, .NET, Metro and UApp)"},{"categories":["Japan"],"date":"2016-02-25T00:00:00Z","description":"2016-02-25","keywords":null,"link":"/blog/2016-02-25-apply-nanaco-point-card/","tags":null,"text":"After six or so years living in Japan, I just got mine today. Yeah, I know‚Ä¶ GaijinPot has an article about point cards from three of the most common convenience stores in the country. I already have Lawsons Ponta card and Family Marts T-Point ages ago but for some reason, I actually havent heard of 7-11s Nanaco. Or maybe I did but I probably didnt bother. But now that I have a Seven Bank account, I noticed from their online bank access site an option to register to their point service. Unsurprisingly, they offer Nanaco points to almost all of my bank transactions (deposits, withdrawals, fund transfers, etc.) which I think is brilliant. But I have to have a Nanaco point card in the first place before I can register. It also said that I can get the card from any 7-11 store. I just have to fill up some application form (surely in Japanese). Or, I can also fill up the application form online, print it and submit to any 7-11 store. I went for the later. Nanaco official website is in Japanese. So Google Translate is your friend here. My Chrome browser is set to auto-translate so Im good to go. Go to this link, scroll down a bit, and press the big red button. It looks like this: It will open a new window with some more Japanese instructions. Scroll down to the very bottom and click the left button. The next page will be the actual form. You will need to input personal information such as your name, birthday, address (both Japanese and Katakana), phone number, etc. I had a few attempts because the password field only accepts numbers (although Google Translation said ‚Äúalphanumeric‚Äù. Oh well). Then click the Next button (the only button) at the very bottom. If you got it right, you will receive a confirmation email containing a link to the document to be printed. It will look like this: Print it and submit to any 7-11 store. The card will cost you 300 yen. After payment, you will get your card right away.","title":"How to apply for a Nanaco point card"},{"categories":["Tech"],"date":"2016-02-24T00:00:00Z","description":"2016-02-24","keywords":null,"link":"/blog/2016-02-24-setup-git-wordpress-bluehost/","tags":["git","wordpress","bluehost"],"text":"I‚Äôm not sure if this is the ‚Äúproper‚Äù way to do it. Well, it sort of works for me at the moment so I thought I‚Äôll share it here. I did use FTP at first (using FileZilla) but I didnt really like the workflow. Host: BlueHost shared account Client: Windows 10 Prerequisites SSH/Shell Access should be enabled. I enabled this from my cPanel -\u003e SSH/Shell Access menu. Server/Host side SSH to BlueHost host. By default, my website was installed inside ~/public_html folder. I created a new folder named ~/www-checkout. This will be my new ‚Äúlive‚Äù website. I copied everything from ~/public_html to ~/www-checkout. $ cp -rv ~/public_html/ ~/www-checkout/ I renamed my ~/public_html to ~/public_html_original. Just for backup. $ mv ~/public_html ~/public_html_original Then I created a symbolic link named ~/public_html that points to ~/www-checkout. $ ln -s ~/www-checkout ~/public_html Then I created my main git repository folder named ~/www.git. $ mkdir www.git $ cd www.git $ git --bare init Then I added a post-receive script inside hooks folder that will do a checkout to ~/www-checkout every time the repository is updated. $ cd hooks $ touch post-receive $ chmod 755 post-receive Contents of post-receive (edited using vim) #!/bin/sh GIT_WORK_TREE=~/www-checkout git checkout -f Then I created a new ‚Äúwork‚Äù folder named ~/www-work for my initial commit, cloned the still empty git repository, then copied the contents of ~/www-checkout. $ cd \u0026\u0026 mkdir www-work $ cd www-work $ git clone ~/www.git Before the commit, I deleted the files that I thought should not be included in the source. wp-content/plugins wp-content/upgrade wp-content/uploads (Optional) Edit source files. Commit source files. $ git add --all $ git commit -a -m \"Initial commit.\" $ git push -u origin master Thats it. Now to the client side. Client side I installed Git for Windows from here. The I created a working folder, right-click -\u003e Git Bash Here menu (I checked the context menu during installation, which by default, was already checked anyway). Clone the repository. $ git clone \u003cusername\u003e@\u003cdomain\u003e:~/www.git I now have a working source copy in my Windows client machine! Disadvantages? One thing I can think of is that every edit goes directly to live folder. So I dont have a sort of ‚Äústaging‚Äù server for me to test before putting changes to live. Both a blessing and a curse, I think. But then again, this is just a simple blog, using a free theme, so I dont think I will be doing any massive changes to the source code anyway.","title":"How I setup git for my WordPress installation in BlueHost"},{"categories":["Tech"],"date":"2016-02-22T00:00:00Z","description":"2016-02-22","keywords":null,"link":"/blog/2016-02-22-ssh-from-windows-to-linux-server/","tags":["ssh"],"text":"At the moment, I‚Äôm still in the process of setting up my home network server. I‚Äôll be taking notes about my progress here so this might be a series of related posts. I‚Äôm using an Ubuntu 14.04 Server (headless). Client is a Windows 10 machine. The simple way Server side Install ssh. How to do this depends on what distro you are using. For Ubuntu, you can do an apt-get install ssh. I didnt have to do this however as I installed it during the server setup. Client side Install Putty. I suggest you download the installer. Open Putty, input your server IP under Host Name (or IP address), and click Open. Enter your server login name and password. But entering server credentials every time you login is really a bit of a hassle, isnt it? Enter public/private keys. The simple and convenient way Server side Create a .ssh folder in you home folder if you havent got one. Then cd to your .ssh folder. Generate a public/private key pair inside .ssh folder. $ ssh-keygen -t rsa -b 4096 -f \u003cfilename\u003e You can choose to add a passphrase, but I didnt since the point here really is to not type anything during login (you still have to enter the passphrase every time you login if you provide one). This will generate two files: the private key with the name \u003cfilename\u003e and the public key \u003cfilename\u003e.pub. Append the public key (\u003cfilename\u003e.pub) to ~/.ssh/authorized_keys. $ cat \u003cfilename\u003e.pub \u003e\u003e authorized_keys $ chmod -R 600 ~/.ssh Copy the private key to your Windows client. Client side Install Putty. I suggest you download the installer. Generate a .ppk file from the private key using PuttyGen. Load the copied private key then click Save private key. Setup Putty to use the .ppk file. Add auto-login username under Connection -\u003e Data. Add the .ppk file to Private key file for authentication under Connection -\u003e SSH -\u003e Auth. Go back to Sessions, enter your servers IP address, add a session name under Saved Sessions and save. Try logging in to your server using the saved session details from #3. You should be able to now without typing your server credentials.","title":"SSH from a Windows client to a Linux server"},{"categories":["general"],"date":"2016-02-21T00:00:00Z","description":"2016-02-21","keywords":null,"link":"/blog/2016-02-21-welcome-to-my-blog/","tags":["welcome","blog"],"text":"Hey, there! Welcome to my blog. I hope you enjoy reading the stuff in here. Nothing fancy, really. Just bits and bobs about tech and random topics. Enjoy!","title":"Welcome to my blog!"}]