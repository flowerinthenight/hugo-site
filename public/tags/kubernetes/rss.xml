













    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    







    

    






<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"  xml:lang="en"  xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        
            

            
                
            

            
                <link href="http://localhost:1313/tags/kubernetes/atom.xml" rel="self" type="application/atom+xml"/>
            
        
            

            
                
            

            
                <link href="http://localhost:1313/tags/kubernetes/" rel="alternate" type="text/html"/>
            
        
            

            

            
                <link href="http://localhost:1313/tags/kubernetes/rss.xml" rel="alternate" type="application/rss+xml"/>
            
        

        

        
            <copyright>© Flowerinthenight, 2016-2024. All rights reserved.</copyright>
        

        <description>Recent content</description>

        
            <language>en</language>
        

        

        <link>http://localhost:1313/tags/kubernetes/</link>

        
            <managingEditor>example@example.com (John Doe)</managingEditor>
        

        <title>Kubernetes · Tags · About</title>

        
            <webMaster>example@example.com (John Doe)</webMaster>
        

        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>This post is somewhat related to a <a href="https://flowerinthenight.com/blog/2018/03/31/access-pods-k8s">previous post</a> about accessing k8s services using nginx reverse proxy. Let&rsquo;s try to add a simple basic authentication to these services at the proxy level. Now, this may come in handy in non-production environments but at the very least, make sure that you are doing this over HTTPS as basic authentication credentials are not encrypted.</p>
<p>We will be using the <code>htpasswd</code> tool to generate our passwords. In Ubuntu, you can install this using the following command:</p>
<p>{% highlight shell %}
$ sudo apt-get install apache2-utils
{% endhighlight %}</p>
<p>Let&rsquo;s generate our password file:</p>
<p>{% highlight shell %}
$ htpasswd -c passfile user1
New password:
Re-type new password:
Adding password for user user1
$ cat passfile
user1:$apr1$c/7lb2VS$SQ9pPJ8XfNpPH.jmnHRsE0
{% endhighlight %}</p>
<p>Let&rsquo;s add a config map to our previous YAML file and enable basic authentication to <code>svc1</code> only:</p>
<p>{% highlight ruby %}
apiVersion: v1
kind: ConfigMap
metadata:
name: basicauth
data:
htpasswd: |
# generate: $ htpasswd -c {file} username (then input password)
user1:$apr1$c/7lb2VS$SQ9pPJ8XfNpPH.jmnHRsE0</p>
<hr>
<p>apiVersion: v1
kind: ConfigMap
metadata:
name: serviceproxy-conf
data:
serviceproxy.conf: |
server {
listen 80;
server_name development.mobingi.com;
resolver kube-dns.kube-system.svc.cluster.local valid=10s;</p>
<pre><code>  location ~ ^/svc1/(.*)$ {
    auth_basic &quot;mobingi&quot;;
    auth_basic_user_file /etc/serviceproxy/htpasswd;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $remote_addr;
    proxy_set_header Host $host;
    rewrite ^/svc1/(.*)$ /$1 break;
    proxy_pass &quot;http://svc1.default.svc.cluster.local&quot;;
    proxy_http_version 1.1;
  }

  location ~ ^/svc2/(.*)$ {
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $remote_addr;
    proxy_set_header Host $host;
    rewrite ^/svc2/(.*)$ /$1 break;
    proxy_pass &quot;http://svc2.default.svc.cluster.local&quot;;
    proxy_http_version 1.1;
  }

  # root health check requirement in GKE ingress
  location / {
    return 200 'healthy\n';
  }
}
</code></pre>
<hr>
<p>apiVersion: apps/v1
kind: Deployment
metadata:
name: serviceproxy
spec:
replicas: 1
revisionHistoryLimit: 3
selector:
matchLabels:
app: serviceproxy
template:
metadata:
labels:
app: serviceproxy
spec:
containers:
- name: nginx
image: nginx:1.13
ports:
- containerPort: 80
volumeMounts:
- name: config-volume
mountPath: /etc/nginx/conf.d/
- name: htpasswd
mountPath: /etc/serviceproxy/
volumes:
- name: config-volume
configMap:
name: serviceproxy-conf
items:
- key: serviceproxy.conf
path: serviceproxy.conf
- name: htpasswd
configMap:
name: basicauth
items:
- key: htpasswd
path: htpasswd</p>
<hr>
<p>apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
name: serviceproxy-hpa
&hellip;
{% endhighlight %}</p>
<p>You should now be able to access <code>svc1</code> using your username:password.</p>
<p>{% highlight shell %}
$ curl -u user1:password <a href="https://development.mobingi.com/svc1/some-endpoint">https://development.mobingi.com/svc1/some-endpoint</a>
{% endhighlight %}</p>
<p>That&rsquo;s it!</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2019-01-31-nginx-basicauth-k8s/</guid>

                
                    <link>http://localhost:1313/blog/2019-01-31-nginx-basicauth-k8s/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>Using nginx basic authentication in Kubernetes</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>Updated 2020/01/27:</p>
<ul>
<li>Support for forwarding to deployments and services</li>
</ul>
<p>Original post:</p>
<p>I recently uploaded a tool to GitHub that wraps <code>kubectl port-forward</code> command and supports port-forwarding to multiple pods. It&rsquo;s called <a href="https://github.com/flowerinthenight/kubepfm"><code>kubepfm</code></a>. I use this tool heavily in my development work. You can check out the code <a href="https://github.com/flowerinthenight/kubepfm">here</a>. You might find this useful.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2020-01-27-kubepfm-update/</guid>

                
                    <link>http://localhost:1313/blog/2020-01-27-kubepfm-update/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>Update to kubepfm, a kubectl port-forward wrapper for multiple pods</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>Updated 2019/02/20:</p>
<ul>
<li>Support for namespaces <a href="https://github.com/flowerinthenight/kubepfm/issues/1">https://github.com/flowerinthenight/kubepfm/issues/1</a></li>
<li>Improved handling of input patterns</li>
</ul>
<p>Original post:</p>
<p>I recently uploaded a tool to GitHub that wraps <code>kubectl port-forward</code> command and supports port-forwarding to multiple pods. It&rsquo;s called <a href="https://github.com/flowerinthenight/kubepfm"><code>kubepfm</code></a>. I use this tool heavily in my development work. You can check out the code <a href="https://github.com/flowerinthenight/kubepfm">here</a>. You might find this useful.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2019-02-20-kubepfm-update/</guid>

                
                    <link>http://localhost:1313/blog/2019-02-20-kubepfm-update/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>Update to kubepfm, a kubectl port-forward wrapper for multiple pods</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<h2 id="overview">Overview</h2>
<p>This post will walk through the steps on how we provisioned our production Kubernetes cluster <a href="https://twitter.com/mobingi">@mobingi</a>. Some of the bits here are already automated in our case but I will try to include as much details as I can.</p>
<p>Our goals would be the following:</p>
<ul>
<li>Provision a Kubernetes cluster on AWS using <a href="https://github.com/kubernetes/kops">kops</a>.</li>
<li>The cluster will have two autoscaling groups: one for on-demand, one for spot instances.</li>
<li>It&rsquo;s going to be a gossip-based cluster.</li>
<li>RBAC is enabled in the cluster.</li>
</ul>
<p>There is no need to rewrite the otherwise excellent installation instructions written in the <a href="https://github.com/kubernetes/kops/blob/master/docs/aws.md">kops wiki</a>. Basically, we can just follow the setup instructions there. The instructions in this post will be specific to our goals above.</p>
<h2 id="gossip-based-cluster">Gossip-based cluster</h2>
<p>We will skip the DNS configuration section as we are provisioning a gossip-based cluster, meaning, the cluster name will be something like <code>&quot;&lt;some-name&gt;.k8s.local&quot;</code>. We will be using <code>&quot;mycluster.k8s.local&quot;</code> as our cluster name.</p>
<h2 id="ssh-keypair">SSH keypair</h2>
<p>We will create the keypair in AWS under EC2 -&gt; Key Pairs -&gt; Create Key Pair. We need this keypair when we need to ssh to our cluster nodes. After saving the keypair somewhere, we will generate the public key using the following command:</p>
<p>{% highlight shell %}
$ ssh-keygen -y -f mycluster.pem &gt; mycluster.pem.pub
{% endhighlight %}</p>
<h2 id="create-the-cluster">Create the cluster</h2>
<p>At this point, we should already have our environment variables set, mainly <code>NAME</code> and <code>KOPS_STATE_STORE</code>. To create the cluster:</p>
<p>{% highlight shell %}</p>
<h1 id="im-from-japan-so-im-using-ap-northeast-1-tokyo">I&rsquo;m from Japan so I&rsquo;m using ap-northeast-1 (Tokyo)</h1>
<p>$ kops create cluster <br>
&ndash;ssh-public-key mycluster.pem.pub
&ndash;zones ap-northeast-1a,ap-northeast-1c <br>
&ndash;authorization RBAC <br>
&ndash;cloud aws ${NAME} <br>
&ndash;yes
{% endhighlight %}</p>
<p>It will take some time before the cluster will be ready. To validate the cluster:</p>
<p>{% highlight shell %}
$ kops validate cluster
Using cluster from kubectl context: mycluster.k8s.local</p>
<p>Validating cluster mycluster.k8s.local</p>
<p>INSTANCE GROUPS
NAME                    ROLE    MACHINETYPE     MIN     MAX     SUBNETS
master-ap-northeast-1a  Master  m3.medium       1       1       &hellip;
nodes                   Node    t2.medium       2       2       &hellip;</p>
<p>NODE STATUS
NAME                                                    ROLE    READY
ip-x-x-x-x.ap-northeast-1.compute.internal              master  True
ip-x-x-x-x.ap-northeast-1.compute.internal              node    True
ip-x-x-x-x.ap-northeast-1.compute.internal              node    True
&hellip;</p>
<p>Your cluster mycluster.k8s.local is ready
{% endhighlight %}</p>
<p>Notice that we only have one instance for our master. We can also opt to have a highly available master using <a href="https://github.com/kubernetes/kops/blob/master/docs/commands.md#other-interesting-modes">these options</a>, which is generally recommended for production clusters. Based on our experience though, this single master instance setup is good enough for development and/or staging clusters. There&rsquo;s going to be downtime if master goes down in which case the duration will depend on how long AWS autoscaling group kicks in. During that window, k8s API won&rsquo;t be accessible but the nodes will continue to work, including our deployed applications.</p>
<h2 id="spot-instance-autoscaling-group">Spot instance autoscaling group</h2>
<p>Once the cluster is ready, we will add another instance group for spot instances. The default instance group created in the previous command, named &ldquo;nodes&rdquo;, will be our on-demand group. To add:</p>
<p>{% highlight shell %}
$ kops create ig nodes-spots -subnet ap-northeast-1a,ap-northeast-1c
{% endhighlight %}</p>
<p>You can then edit using the following contents (modify values as needed):</p>
<p>{% highlight ruby %}
apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
creationTimestamp: <some-datetime-value-here>
labels:
kops.k8s.io/cluster: mycluster.k8s.local
name: nodes-spot
spec:
image: kope.io/k8s-1.8-debian-jessie-amd64-hvm-ebs-2017-12-02
machineType: t2.medium
maxPrice: &ldquo;0.02&rdquo;
maxSize: 10
minSize: 2
nodeLabels:
kops.k8s.io/instancegroup: nodes
spot: &ldquo;true&rdquo;
role: Node
subnets:</p>
<ul>
<li>ap-northeast-1a</li>
<li>ap-northeast-1c
{% endhighlight %}</li>
</ul>
<p>We can now update our cluster with the following commands:</p>
<p>{% highlight shell %}</p>
<h1 id="optional-update-our-on-demand-groups-max-size-to-some-number">[optional] update our on-demand group&rsquo;s max size to some number</h1>
<p>$ kops edit ig nodes</p>
<h1 id="optional-preview-the-changes-to-be-applied">[optional] preview the changes to be applied</h1>
<p>$ kops update cluster ${NAME}</p>
<h1 id="actual-cluster-update">actual cluster update</h1>
<p>$ kops update cluster ${NAME} &ndash;yes</p>
<h1 id="optional-check-if-we-need-rolling-update">[optional] check if we need rolling update</h1>
<p>$ kops rolling-update cluster</p>
<h1 id="if-so-add---yes-option">if so, add &ndash;yes option</h1>
<p>$ kops rolling-update cluster &ndash;yes
{% endhighlight %}</p>
<p>We can now validate the cluster to see our changes:</p>
<p>{% highlight shell %}
$ kops validate cluster
Validating cluster mycluster.k8s.local</p>
<p>INSTANCE GROUPS
NAME                    ROLE    MACHINETYPE     MIN     MAX     SUBNETS
master-ap-northeast-1a  Master  m3.medium       1       1       &hellip;
nodes                   Node    t2.medium       2       4       &hellip;
nodes-spot              Node    t2.medium       2       10      &hellip;</p>
<p>NODE STATUS
NAME                                                    ROLE    READY
ip-x-x-x-x.ap-northeast-1.compute.internal              master  True
ip-x-x-x-x.ap-northeast-1.compute.internal              node    True
ip-x-x-x-x.ap-northeast-1.compute.internal              node    True
&hellip;</p>
<p>Your cluster mycluster.k8s.local is ready
{% endhighlight %}</p>
<p>When our cluster was created, kops also has automatically generated our config file for kubectl. To verify:</p>
<p>{% highlight shell %}
$ kubectl cluster-info
Kubernetes master is running at https&hellip;
KubeDNS is running at https&hellip;</p>
<p>To further debug and diagnose cluster problems, use &lsquo;kubectl cluster-info dump&rsquo;.
{% endhighlight %}</p>
<h2 id="setup-cluster-autoscaler">Setup cluster autoscaler</h2>
<p>Clusters created using kops use autoscaling groups, but without scaling policies (at the time of writing). To enable dynamic scaling of our cluster, we will use <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws">cluster autoscaler</a>. Before cluster autoscaler deployment, we need to setup some prerequisites.</p>
<p>First, we need to attach the following permissions to master and nodes IAM role. Go to IAM roles console and add an inline policy to the roles created by kops. Role names would be something like:</p>
<ul>
<li>master.mycluster.k8s.local</li>
<li>nodes.mycluster.k8s.local</li>
</ul>
<p>{% highlight json %}
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;autoscaling:DescribeAutoScalingGroups&rdquo;,
&ldquo;autoscaling:DescribeAutoScalingInstances&rdquo;,
&ldquo;autoscaling:DescribeTags&rdquo;,
&ldquo;autoscaling:SetDesiredCapacity&rdquo;,
&ldquo;autoscaling:TerminateInstanceInAutoScalingGroup&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}
{% endhighlight %}</p>
<p>The latest installation instructions can be found <a href="https://github.com/kubernetes/kops/tree/master/addons/cluster-autoscaler">here</a>. The general idea is to choose the latest yaml file, update it with your own values, and apply the file using kubectl. The readme file also provides a script that does the download and edit for us. We will be using the following command line arguments for our autoscaler:</p>
<p>{% highlight ruby %}
command:</p>
<ul>
<li>./cluster-autoscaler</li>
<li>&ndash;cloud-provider=aws</li>
<li>&ndash;v=4</li>
<li>&ndash;stderrthreshold=info</li>
<li>&ndash;scale-down-delay=5m</li>
<li>&ndash;skip-nodes-with-local-storage=false</li>
<li>&ndash;expander=least-waste</li>
<li>&ndash;nodes=2:4:nodes.mycluster.k8s.local</li>
<li>&ndash;nodes=2:10:nodes-spot.mycluster.k8s.local
{% endhighlight %}</li>
</ul>
<p>You should update the last two line with your own autoscaling group min/max values. Finally, we deploy our autoscaler with:</p>
<p>{% highlight shell %}
$ kubectl create -f cluster-autoscaler.yml
deployment &ldquo;cluster-autoscaler&rdquo; created
{% endhighlight %}</p>
<p>That&rsquo;s it. You may also want to install <a href="https://github.com/kubernetes/kops/blob/master/docs/addons.md#installing-kubernetes-addons">these addons</a> if you like.</p>
<h2 id="ssh-to-a-node">SSH to a node</h2>
<p>{% highlight shell %}</p>
<h1 id="sample-ips-only">sample ip&rsquo;s only</h1>
<p>$ kubectl get node -o wide
NAME                           STATUS    ROLES     AGE   VERSION   EXTERNAL-IP  OS-IMAGE                      &hellip;
ip-x.x.x.x.ap-northeast-1&hellip;   Ready     master    1d    v1.8.6    1.2.3.4      Debian GNU/Linux 8 (jessie)   &hellip;
ip-x.x.x.x.ap-northeast-1&hellip;   Ready     node      1d    v1.8.6    1.2.3.5      Debian GNU/Linux 8 (jessie)   &hellip;
ip-x.x.x.x.ap-northeast-1&hellip;   Ready     node      1d    v1.8.6    1.2.3.6      Debian GNU/Linux 8 (jessie)   &hellip;
ip-x.x.x.x.ap-northeast-1&hellip;   Ready     node      1d    v1.8.6    1.2.3.7      Debian GNU/Linux 8 (jessie)   &hellip;
ip-x.x.x.x.ap-northeast-1&hellip;   Ready     node      1d    v1.8.6    1.2.3.8      Debian GNU/Linux 8 (jessie)   &hellip;</p>
<h1 id="default-username-is-admin">default username is <code>admin</code></h1>
<p>$ ssh -i mycluster.pem <a href="mailto:admin@1.2.3.4">admin@1.2.3.4</a>
{% endhighlight %}</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2018-01-24-k8s-kops-aws/</guid>

                
                    <link>http://localhost:1313/blog/2018-01-24-k8s-kops-aws/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>Running Kubernetes on AWS using kops</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I posted an <a href="https://tech.mobingi.com/2018/12/05/mochi-k8s-repo.html">article</a> for <a href="https://twitter.com/mobingi">@mobingi</a> about its Kubernetes clusters repo. Please check it out. Thanks.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2018-12-27-mochi-k8s-repo/</guid>

                
                    <link>http://localhost:1313/blog/2018-12-27-mochi-k8s-repo/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>Mobingi’s Kubernetes clusters repo</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I recently uploaded a tool to GitHub that wraps <code>kubectl port-forward</code> command and supports port-forwarding to multiple pods. It&rsquo;s called <a href="https://github.com/flowerinthenight/kubepfm"><code>kubepfm</code></a>. I use this tool heavily in my development work. You can check out the code <a href="https://github.com/flowerinthenight/kubepfm">here</a>. You might find this useful.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2018-07-24-kubepfm/</guid>

                
                    <link>http://localhost:1313/blog/2018-07-24-kubepfm/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>kubepfm, a kubectl port-forward wrapper for multiple pods</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>It took me a while to make this work. I hope this will help someone out there who also is struggling with the same problem.</p>
<p>We use DigiCert as our SSL certificate provider. The package I received contained three files:</p>
<ul>
<li>a keyfile, filename.key</li>
<li>a certificate file, filename.crt</li>
<li>an intermediate certificate file, DigiCertCA.crt</li>
</ul>
<p>I had to combine the two certificate files into a single file. I didn&rsquo;t really check the order but I appended the intermediate certificate to my certificate file. Something like this:</p>
<p>{% highlight shell %}
$ cp filename.crt tls.crt
$ cat DigiCertCA.crt &raquo; tls.crt
$ cp filename.key tls.key
$ kubectl create secret tls mytls &ndash;key tls.key &ndash;cert tls.crt
{% endhighlight %}</p>
<p>I was able to successfully use the secret in a GCE Ingress:</p>
<p>{% highlight ruby %}
apiVersion: extensions/v1beta1
kind: Ingress
&hellip;
spec:
tls:</p>
<ul>
<li>secretName: mytls
backend:
serviceName: myservice
servicePort: 80
&hellip;
{% endhighlight %}</li>
</ul>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2018-02-20-k8s-tls-digicert/</guid>

                
                    <link>http://localhost:1313/blog/2018-02-20-k8s-tls-digicert/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>Creating a Kubernetes TLS secret using certificates from DigiCert</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I recently came across the <a href="https://github.com/hashicorp/memberlist">hashicorp/memberlist</a> library while browsing GitHub and I thought it would be a good replacement for <a href="https://github.com/flowerinthenight/hedge">hedge</a>&rsquo;s internal member tracking logic. It seems to be widely used (thus more battle-tested) as well. I was quite excited as I always thought that hedge&rsquo;s equivalent logic is too barebones and untested outside of our use cases. It works just fine for its current intended purpose but I&rsquo;ve been hesitating to build on top of it until I can really say that it&rsquo;s stable enough. With memberlist, it might just be what I needed.</p>
<p>After about a month of testing, I think it didn&rsquo;t really turn out quite well in the end. It is stable enough for deployments that are not spike-y in terms of workloads (frequent scaling up/down). Or if I set min = max in the <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a>. In these cases, memberlist can consistently track members just fine. What&rsquo;s better is that it works even in multiple deployments in the same namespace which I thought was brilliant. For example, if I have a deployment <code>app1</code> set to 10 pods in the <code>default</code> namespace using memberlist&rsquo;s default port numbers, and then I deploy another set, say, <code>app2</code>, within the same namespace using the same ports, app1&rsquo;s memberlist can track its 10 member pods just fine while app2&rsquo;s memberlist is also separated. But when applied to my use case, which has a minimum pod of 2 and a max of 150, with frequent scale up/down frequency depending on load, it can&rsquo;t seem to keep up. The potential for <a href="https://en.wikipedia.org/wiki/Byzantine_fault">Byzantine faults</a> is just too high: i.e. in a 50-pod scale, memberlist can end up having 2 groups of m-pods and n-pods where m+n=50. Very rarely, it can even go up to 3 groups.</p>
<p>I am a little frustrated. I really wanted it to work; I even attempted to update memberlist to incorporate hedge&rsquo;s logic but it was too much for now, with my schedule. So now, back to the old one. By the way, the current logic is fairly rudimentary: all members in the cluster/group send a liveness heartbeat to the leader and the leader broadcasting the final list of members to all via hedge&rsquo;s broadcast mechanism. CPU usage between the two is fairly similar depending on the sync timeout.</p>
<p>I&rsquo;ve been trying to improve hedge&rsquo;s member tracking system as I want to build a distributed in-memory cache within hedge itself. Most of the available ones are <a href="https://raft.github.io/">Raft</a>-based, and I still haven&rsquo;t figured out how to make Raft work in the same deployment configuration.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2023-04-28-hedge-memberlist/</guid>

                
                    <link>http://localhost:1313/blog/2023-04-28-hedge-memberlist/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>Attempt to replace hedge’s member tracking with hashicorp/memberlist</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I uploaded an automation-friendly, highly-scalable, and scriptable API/generic testing tool built to run on <a href="https://kubernetes.io/">Kubernetes</a>. It&rsquo;s called <code>oops</code> and you can find the code <a href="https://github.com/flowerinthenight/oops/">here</a>. Maybe it will be useful to anybody out there.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2020-10-31-oops/</guid>

                
                    <link>http://localhost:1313/blog/2020-10-31-oops/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>An automation-friendly, highly-scalable, and scriptable testing tool</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>At work, for a couple of months now, we&rsquo;ve been using <a href="https://www.getambassador.io/">Ambassador</a> as our main API gateway to our k8s services. We also have our own authorization service that uses Ambassador&rsquo;s <a href="https://www.getambassador.io/reference/services/auth-service">AuthService</a> mechanism. Recently, we&rsquo;ve had services that needed CORS support and although Ambassador has features that support the <a href="https://www.getambassador.io/reference/cors">enabling of CORS</a>, we had to update our authorization service to handle CORS-related requests. Instead of doing this, we tried adding the CORS support at the proxy level (nginx). I&rsquo;ve wrote about this topic <a href="https://flowerinthenight.com/blog/2018/03/31/access-pods-k8s">here</a> and <a href="https://flowerinthenight.com/blog/2019/01/31/nginx-basicauth-k8s">here</a>.</p>
<p>In the example below, the CORS support is added under the location <code>/svc2/</code>.</p>
<h2 id="-highlight-ruby-">{% highlight ruby %}</h2>
<p>apiVersion: v1
kind: ConfigMap
metadata:
name: serviceproxy-conf
data:
serviceproxy.conf: |
server {
listen 80;
server_name development.mobingi.com;
resolver kube-dns.kube-system.svc.cluster.local valid=10s;</p>
<pre><code>  location ~ ^/svc1/(.*)$ {
    auth_basic &quot;mobingi&quot;;
    auth_basic_user_file /etc/serviceproxy/htpasswd;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $remote_addr;
    proxy_set_header Host $host;
    rewrite ^/svc1/(.*)$ /$1 break;
    proxy_pass &quot;http://svc1.default.svc.cluster.local&quot;;
    proxy_http_version 1.1;
  }

  location ~ ^/svc2/(.*)$ {
    # Ref: https://enable-cors.org/server_nginx.html
    if ($request_method = 'OPTIONS') {
      add_header 'Access-Control-Allow-Origin' '*';
      add_header 'Access-Control-Allow-Methods' 'POST, OPTIONS';
      # Custom headers and headers various browsers *should* be OK with but aren't.
      add_header 'Access-Control-Allow-Headers' 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization';
      # Tell client that this pre-flight info is valid for 20 days.
      add_header 'Access-Control-Max-Age' 1728000;
      add_header 'Content-Type' 'text/plain; charset=utf-8';
      add_header 'Content-Length' 0;
      return 204;
    }
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $remote_addr;
    proxy_set_header Host $host;
    rewrite ^/svc2/(.*)$ /$1 break;
    proxy_pass &quot;http://svc2.default.svc.cluster.local&quot;;
    proxy_http_version 1.1;
  }

  # root health check requirement in GKE ingress
  location / {
    return 200 'healthy\n';
  }
}

{..redacted..}
</code></pre>
<p>{% endhighlight %}</p>
<p>With that said, be aware of the <a href="https://www.nginx.com/resources/wiki/start/topics/depth/ifisevil/">potential problems of using if in nginx</a>.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2019-05-26-cors-nginx-k8s/</guid>

                
                    <link>http://localhost:1313/blog/2019-05-26-cors-nginx-k8s/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>Adding CORS support to nginx proxy in Kubernetes</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>At <a href="https://mobingi.com">Mobingi</a>, when we are developing services that run on Kubernetes, we generally use <a href="https://github.com/kubernetes/minikube">Minikube</a> or <a href="https://blog.docker.com/2018/01/docker-mac-kubernetes/">Kubernetes in Docker for Mac</a>. We also have a cluster that runs on <a href="https://cloud.google.com/kubernetes-engine/">GKE</a> that we use for development. In this post, I will share how we access some of the services that are running on our development cluster.</p>
<h2 id="using-kubectl-port-forward">Using kubectl port-forward</h2>
<p>Using <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">kubectl port-forward</a> is probably the cheapest and the most straightforward. For example, if I want to access a cluster service <code>svc1</code> through my localhost, I use <code>kubectl port-forward</code> like this:</p>
<p>{% highlight shell %}
$ kubectl get pod
NAME                            READY     STATUS    RESTARTS   AGE
svc1-66dd787767-d6b22           2/2       Running   0          7d
svc1-66dd787767-ks92f           2/2       Running   0          7d
svc2-578786c554-rlw2w           2/2       Running   0          7d</p>
<h1 id="this-will-connect-to-the-first-pod-we-have-two-available">This will connect to the first pod (we have two available):</h1>
<p>$ kubectl port-forward <code>kubectl get pod --no-headers=true -o \ custom-columns=:metadata.name | grep svc1 | head -n1</code> 8080:8080
Forwarding from 127.0.0.1:8080 -&gt; 8080
{% endhighlight %}</p>
<p>The left <code>8080</code> is my local port, the right <code>8080</code> is the pod port where svc1 is running.</p>
<p>One thing to note with <code>kubectl port-forward</code> through is that it won&rsquo;t disconnect automatically even when the pod is restarted, say for example, due to an update from CI. I have to restart the command by doing a Ctrl+C and then rerun.</p>
<h2 id="exposing-the-service-using-nodeport-or-loadbalancer">Exposing the service using NodePort or LoadBalancer</h2>
<p>This part is probably the easiest to setup. You can check the details from the Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types">documentation</a>. But you have to be careful though, especially with load balancers. These are not cheap. We have gone with this route during our early Kubernetes days and we ended up with a lot of load balancers. This was when our clusters were still in AWS. In AWS, (I&rsquo;m not sure if it is still the case now) when you specify <code>LoadBalancer</code> as service type, a classic load balancer will be provisioned for your service. That means one load balancer per exposed service!</p>
<p>When we moved to GKE, we started using <a href="https://github.com/kubernetes/ingress-gce">GLBC</a> which uses an L7 load balancer via the Ingress API. This improved our costs a little bit since GLBC can support up to five backend services per load balancer using paths. The slight downside was that Ingress updates were a bit slow. It&rsquo;s not a big deal though since it&rsquo;s only in the development cluster and we use blue/green deployment in production. But still, some updates can take up to ten minutes.</p>
<h2 id="using-nginx-as-a-reverse-proxy-to-cluster-services">Using nginx as a reverse proxy to cluster services</h2>
<p>In our quest to further minimize costs, we are currently using <a href="https://www.nginx.com/">nginx</a> as our way of exposing services. We provisioned a single Ingress that points to an nginx service which serves as a reverse proxy to our cluster services. This was the cheapest for us as we only have one load balancer for all services. And updating the nginx reverse proxy service takes only a few seconds. So far, this worked for us with no significant problems for the past couple of months.</p>
<p>Here&rsquo;s an example of an nginx reverse proxy service:</p>
<p>{% highlight ruby %}
apiVersion: v1
kind: ConfigMap
metadata:
name: serviceproxy-conf
data:
serviceproxy.conf: |
server {
listen 80;
server_name development.mobingi.com;
resolver kube-dns.kube-system.svc.cluster.local valid=10s;</p>
<pre><code>    location ~ ^/svc1/(.*)$ {
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header Host $host;
        rewrite ^/svc1/(.*)$ /$1 break;
        proxy_pass &quot;http://svc1.default.svc.cluster.local&quot;;
        proxy_http_version 1.1;
    }

    location ~ ^/svc2/(.*)$ {
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header Host $host;
        rewrite ^/svc2/(.*)$ /$1 break;
        proxy_pass &quot;http://svc2.default.svc.cluster.local&quot;;
        proxy_http_version 1.1;
    }

    # root health check requirement in GKE ingress
    location / {
        return 200 'healthy\n';
    }
}
</code></pre>
<hr>
<p>apiVersion: apps/v1
kind: Deployment
metadata:
name: serviceproxy
spec:
replicas: 1
revisionHistoryLimit: 3
selector:
matchLabels:
app: serviceproxy
template:
metadata:
labels:
app: serviceproxy
spec:
containers:
- name: nginx
image: nginx:1.13
ports:
- containerPort: 80
volumeMounts:
- name: config-volume
mountPath: /etc/nginx/conf.d/
volumes:
- name: config-volume
configMap:
name: serviceproxy-conf
items:
- key: serviceproxy.conf
path: serviceproxy.conf</p>
<hr>
<p>apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
name: serviceproxy-hpa
namespace: default
spec:
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: serviceproxy
minReplicas: 1
maxReplicas: 10
targetCPUUtilizationPercentage: 80</p>
<hr>
<p>apiVersion: v1
kind: Service
metadata:
name: serviceproxy
spec:
type: NodePort
ports:</p>
<ul>
<li>name: http
protocol: TCP
port: 80
targetPort: 80
selector:
app: serviceproxy
{% endhighlight %}</li>
</ul>
<p>In this example, all services, mainly <code>svc1</code> and <code>svc2</code>, are running in the <code>default</code> namespace. Save this as service.yaml and deploy:</p>
<p>{% highlight shell %}
$ kubectl create -f service.yaml
{% endhighlight %}</p>
<p>A sample Ingress controller for the reverse proxy service:</p>
<p>{% highlight ruby %}
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
name: serviceproxy-ingress
annotations:
kubernetes.io/ingress.class: &ldquo;gce&rdquo;
spec:
tls:</p>
<ul>
<li>secretName: mobingi-tls
rules:</li>
<li>host: development.mobingi.com
http:
paths:
<ul>
<li>backend:
serviceName: serviceproxy
servicePort: 80
{% endhighlight %}</li>
</ul>
</li>
</ul>
<p>Save this as ingress.yaml and deploy:</p>
<p>{% highlight shell %}
$ kubectl create -f ingress.yaml
{% endhighlight %}</p>
<p>After everything is ready (Ingress provisioning takes some time), you should be able to access <code>svc1</code> through <code>https://development.mobingi.com/svc1/some-endpoint</code>, <code>svc2</code> through <code>https://development.mobingi.com/svc2/another-endpoint</code>, etc. Of course, you have to point your domain to your Ingress load balancer&rsquo;s IP address which you can see using the following command:</p>
<p>{% highlight shell %}
$ kubectl get ingress serviceproxy-ingress
NAME                   HOSTS                     ADDRESS          PORTS     AGE
serviceproxy-ingress   development.mobingi.com   1.2.3.4          80, 443   91d
{% endhighlight %}</p>
<p>If you&rsquo;re wondering how to setup the TLS portion, you can refer to my previous <a href="https://flowerinthenight.com/blog/2018/02/20/k8s-tls-digicert">post</a> about the very subject.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,0001-01-01:/blog/2018-03-31-access-pods-k8s/</guid>

                
                    <link>http://localhost:1313/blog/2018-03-31-access-pods-k8s/</link>
                

                
                    <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
                

                
                    <title>Accessing services in Kubernetes</title>
                
            </item>
        
    </channel>
</rss>
